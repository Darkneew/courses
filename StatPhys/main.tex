\documentclass[a4paper]{book}

% packages % 
\usepackage[utf8]{inputenc} 
\usepackage{fvextra}
\usepackage{csquotes}
\usepackage[french, italian, spanish, english]{babel}
\usepackage[T1]{fontenc}   
\usepackage{color}  
\usepackage{amsmath, dsfont, amssymb, amsthm, stmaryrd, mathrsfs}
\usepackage[style=alphabetic]{biblatex}
\usepackage{enumitem}
\usepackage[hidelinks]{hyperref}

% graphics %
\usepackage{graphicx}
\graphicspath{ {./images/} }

% environments %
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{example}{Example}



% bibliography %
\bibliography{bibliography} 

\begin{document}

% title %
\title{Advanced statistical physics and new applications}
\author{Buisine Léo\\Ecole Normale Superieure of Paris}
\maketitle

\tableofcontents

\chapter{Introduction}

Frédérid van Wijland: fvw@u-paris.fr \newline
studies statistical physics / field theory applied to stat phys, glass, active matter, \dots \par \medskip 

We don't believe it right now, but this course will be useful to us. \par \medskip 

We will have the notes allowed in the exam, but only hand written notes. \par \medskip 

The lecture notes are oversized, to help us find things we like, to feed us with interesting things. However, only what we see during the course is to be known in the exam. 

\chapter{Statistical dynamics}

We know stat mechs in equilibrium (stationnary and reversible?). We want to do stat mechs including time. \par \bigskip 

There is always a level of description of our system described by what is called the "Master equation". 

\section{The physics behind a master equation}

\subsection{Phase space and Liouville equation}
Imagine a physical system described by its phase space coordinates $\Gamma = \{q, p\}$, and its evolution is governed by Hamilton's equations of motion 
\begin{equation}
    q = \frac{\partial \mathscr{H}}{\partial p}, \quad p = - \frac{\partial \mathscr{H}}{\partial q}
\end{equation}
where $\mathscr{H}$ is the Hamiltonian encoding all dynamical informations of the system. \par \medskip 

Let $\rho(\Gamma, t)$ be the phase space density. Then we have a conservation equation in the phase space 
\begin{equation}
    \partial_t \rho + \partial_\Gamma \cdot (\rho \dot{\Gamma}) = 0
\end{equation}
By expanding, we obtain 
\begin{equation}
    \partial_t \rho = \{\mathscr{H}, \rho\}
\end{equation}
Because $\partial_\Gamma \cdot \dot\Gamma = 0$, the phase space flow is incompressible. This is the Liouville equation. \par \medskip 
Sometimes, we write it 
\begin{equation}
    \partial_t \rho = -i \mathscr{L} \rho
\end{equation}
as a linear operator, where $-i \mathscr{L}\cdot = \{\mathscr{H}, \cdot\}$

\subsection{Projection operator} 
If at the microscopic level some degrees of freedom $c$ have a distribution $\rho(c, t)$, then what happens to a "coarse-grained" quantity 
\begin{equation}
    C(c): \mathbb{P}(\mathscr{C}, t) = \text{Prob}\{C(c) = \mathscr{C} \text{ at time } t\}
\end{equation}
What we know is the evolution of $\rho(c, t)$:
\begin{equation}
    \partial_t \rho = w\rho = \sum_{c'}w_{cc'}\rho(c', t)
\end{equation}
with $w$ some linear operator. How can we find an evolution for $\mathbb{P}(\mathscr{C}, t)$? \par \medskip 

First, we can relate $P$ to $\rho$: 
\begin{equation}
    P(\mathscr{C}, t) = \sum_c \delta_{\mathscr{C}, C(c)}\rho(c, t)
\end{equation}
Idea: 
\begin{equation}
    \mathscr{P}f(c) = \frac{1}{\Omega(C(c))}\sum_{c'}\delta_{C(c), C(c')}f(c')
\end{equation}
where 
\begin{equation}
    \Omega(\mathscr{C}) = \sum_c \delta_{\mathscr{C}, C(c)}
\end{equation}
We can check that $\mathscr{P}^2 = \mathscr{P}$. 
Let $\bar{p}(c, t) = \mathscr{P}p(c, t)$, then one can access $P(\mathscr{C}, t)$ via 
\begin{equation}
    P(\mathscr{C}, t) = \sum_c \delta_{\mathscr{C}, C(c)}\bar{p}(c, t)
\end{equation}
Now we split $p = \bar{p} + q$ with $\bar{p} = \mathscr{P}$, $q = (1-\mathscr{P})p$. and we can start from $\partial_t p = wp$ and thus arrive at 
\begin{equation}
    \begin{aligned}
        &\partial_t \bar{p} = \mathscr{P} w(\bar{p} + q) \\ 
        &\partial_t q = (1- \mathscr{P})w(\bar{p} + q) \Rightarrow \partial_t q - (1-\mathscr{P})wq = (1-\mathscr{P})w\bar{p}
    \end{aligned}
\end{equation}
We solve the equation for $q$, and inject $q$ as a functional of $\bar{p}$ into 
\begin{equation}
    \partial_t\bar{p} = \mathscr{P}w\bar{p} + \mathscr{P}wq
\end{equation}
we get 
\begin{equation}
    \partial_t \bar{p} = \mathscr{P}w\bar{p} + \int_{0}^{t} \bar{d}t' ~ \mathscr{P}we^{(1-\mathscr{P})w(t-t')}(1-\mathscr{P})w\bar{p}(t')
\end{equation}
There is the emergence of a memory loss $e^{(1-\mathscr{P})w(t-t')}$. We can reconstruct an equation for $P(\mathscr{C}, t)$, which has the general form 
\begin{equation}
    \partial_t P = M^{(1)}P + \frac{0}{t}\text{d}t' M^{(2)}P(t')
\end{equation}
where $M^{(2)}$ encodes the memory of the degrees of freedom lost in the description 
\begin{equation}
    \text{"}M^{(2)}(t) \propto e^{(1-\mathscr{P})wt} \text{"}
\end{equation}
If the eigenvalues of $(1-\mathscr{P})wt$ are very large, this means that this memory is quickly lost 
\begin{equation}
    e^{-\frac{t}{\tau}} \sim_{\tau \sim 0} \tau \delta(t)
\end{equation}
and then the equation for $P$ reads 
\begin{equation}
    \partial_t P = M_{\text{eff}}P
\end{equation}

If the physical system of interest is such that its typical time scale are much larger than those characterizing the interactions with the environment, then we end up on a linear, 1st order in time, diff equation for $P$. This is a master equation. \par \medskip 

See lecture notes for more details 

\section{Master equation}

The best book talking of the master equation is the one by N. Van Kampen, stochastic processes... libgen

\subsection{Rates}

We will use the notation $\mathscr{C}$ to refer to "microscopic" configs. Let 
\begin{equation}
    W(\mathscr{C} \rightarrow \mathscr{C}', t)\text{d}t
\end{equation}
be the probability that the system hops from config $\mathscr{C}$ to config $\mathscr{C}'$ between $t$ and $t+\text{d}t$. Then the rate $r(\mathscr{C})$ at which the system escapes config $\mathscr{C}$ is 
\begin{equation}
    r(\mathscr{C}) = \sum_{\mathscr{C}'}W(\mathscr{C}\rightarrow\mathscr{C}')
\end{equation}

Then it is possible to write the evolution equation for $P(\mathscr{C}, t)$: 
\begin{equation}
    P(\mathscr{C}, t+\text{d}t) = P(\mathscr{C}, t)(1- r(\mathscr{C})\text{d}t) + \sum_{\mathscr{C}'}P(\mathscr{C}', t)W(\mathscr{C}'\rightarrow\mathscr{C})\text{d}t
\end{equation}
Implying 
\begin{equation}
    \partial_t P(\mathscr{C}, t) = \sum_{\mathscr{C}'}W(\mathscr{C}'\rightarrow\mathscr{C})P(\mathscr{C}', t) - r(\mathscr{C})P(\mathscr{C}, t)
\end{equation}
This expresses a random walk on a directed graph with vertices $\mathscr{C}$ and weights over edges given by the $W(\mathscr{C}, \mathscr{C}')$'s. 

It is sometimes convenient to introduce 
\begin{equation}
    \begin{aligned}
        &W(\mathscr{C}' \rightarrow \mathscr{C}) \quad \text{if } \mathscr{C} \neq \mathscr{C}' \\ 
        W_{\mathscr{C}\mathscr{C}'} =&\\
        &-r(\mathscr{C}) \quad \text{if }\mathscr{C} = \mathscr{C}
    \end{aligned}
\end{equation}
A posteriori, we see that, of course, there is a probability conservation 
\begin{equation}
    \frac{d}{dt}\sum_{\mathscr{C}}P(\mathscr{C}, t) = \sum_{\mathscr{C}\mathscr{C}'}W_{\mathscr{C}\mathscr{C}'} P(\mathscr{C}', t) = 0
\end{equation}
because 
\begin{equation}
    \forall \mathscr{C}', \quad \sum_\mathscr{C} W_{\mathscr{C}\mathscr{C}'} = 0 = \sum_{\mathscr{C}}\left[W(\mathscr{C}'\rightarrow \mathscr{C}) - r(\mathscr{C})\delta_{\mathscr{C}\mathscr{C}'}\right]
\end{equation}
Let $p_\mathscr{C} = 1$ for all $\mathscr{C}$, then 
\begin{equation}
    \sum_\mathscr{C} p_\mathscr{C} W_{\mathscr{C}\mathscr{C}'} = 0, \quad p^\dagger W = 0
\end{equation}

Hence the $p$ vector is a left eigenvector of $W$ with eigenvalue 0, hence there exists a right eigenvector $P_ss$ with eigenvalue 0. This is describing a steady-state. Let's restrict ourselves to strongly connected graphs of configurations (with irreducible dynamics), so as to avoid stationary probabilities being drained towards a specific subgraph. \par \medskip 

If we write 
\begin{equation}
    P(t + \text{d}t) = (1 + W\text{d}t)P(t)
\end{equation}
we apply the Perron-Frobenius theorem to $M = (1 + W\text{d}t)$, and we can conclude that there exists a positive  number $\rho$ that is an eigenvalue and all others are smaller. 
\begin{equation}
    l = \min_{\mathscr{C}} \sum_{\mathscr{C}'} M^\dagger_{\mathscr{C}\mathscr{C}'} \leq \rho \leq \max_{\mathscr{C}} \sum_{\mathscr{C}'}M^\dagger_{\mathscr{C}\mathscr{C}'} = 1
\end{equation}

Hence 0 is the largest eigenvalue of $W$, and the unique corresponding vector $P_ss$ has all its components of the same sign (we choose +).

\subsection{Averages}
In practice, given a master equation and a physical observable $B(\mathscr{C})$, it is possible to find the evolution of $<B>$:
\begin{equation}
    <B> = \sum_{\mathscr{C}} B(\mathscr{C})P(\mathscr{C}, t)
\end{equation}
such that 
\begin{equation}
    \begin{aligned}
        \frac{\text{d}<B>}{\text{d}t} &= \sum_{\mathscr{C}\mathscr{C}'} B(\mathscr{C})W_{\mathscr{C}\mathscr{C}'}P(\mathscr{C}', t) \\
        &= p^\dagger BWP \\ 
        &= p^\dagger [B, W]P
    \end{aligned}
\end{equation}
and a formal solution reads 
\begin{equation}
    <B(t)> = \sum_{\mathscr{C}\mathscr{C}'}B(\mathscr{C})(e^{Wt})_{\mathscr{C}\mathscr{C}'} P(\mathscr{C}', 0)
\end{equation}
Similarly, 
\begin{equation}
    <A(t_1)B(t_2)> = p^\dagger Ae^{W(t_1, t_2)}Be^{Wt_2}P_{\text{init}}
\end{equation}

\subsection{Trajectories and histories}

Let's see what the existence of rate actually mean: when the system is in state $\mathscr{C}$, then it stays there for a random duration before hopping to configuration $\mathscr{C}'$. \par \medskip 

Let $\Pi(\tau, \mathscr{C})\text{d}\tau$ be the probability to stay in state $\mathscr{C}$ for a duration $\tau$ and to hop to some other state between $\tau$ and $\tau + \text{d}\tau$
\begin{equation}
    \Pi(\tau, \mathscr{C}) = f(\mathscr{C}, \tau)r(\mathscr{C})\text{d}\tau
\end{equation}
where $f$ is the probability to stay a duration $\tau$ in state $\mathscr{C}$. 
\begin{equation}
    \begin{aligned}
        f(\mathscr{C},& \tau + \text{d}\tau) = f(\mathscr{C}, \tau)(1-r(\mathscr{C})\text{d}\tau) \\ 
        f(\mathscr{C},& 0) = 1 \\
        &\quad \Rightarrow f(\mathscr{C}, \tau) = e^{-r(\mathscr{C})\tau}
    \end{aligned}
\end{equation}

Let's look at a typical realization of the Markov process over some time window $[0, t_{\text{obv}}]$.
\begin{equation}
    \mathscr{C}_0 \rightarrow^{\tau_0} \mathscr{C}_1 \rightarrow^{\tau_1} \mathscr{C}_2 \rightarrow^{\tau_2} \dots \rightarrow^{\tau_{K-1}} \mathscr{C}_K\rightarrow^{\tau_{K}} \mathscr{C}_K
\end{equation}
with $K$ the number of hops over $[0, t_{\text{obs}}]$, which is a random variable. So $K$ is random, the $\tau_i$'s are random, and the $\mathscr{C}_i$'s are random. \par \medskip 

The probability of observing this trajectory is 
\begin{equation}
    \begin{aligned}
        \mathscr{P}[\text{traj}] = &P_{\text{init}}(\mathscr{C}_0) \Pi(\tau_0, \mathscr{C}_0) \text{d}\tau_0 \frac{W(\mathscr{C}_0 \rightarrow \mathscr{C}_1)}{r(\mathscr{C}_0)} \Pi(\tau_1, \mathscr{C}_1) \text{d}\tau_1 \frac{W(\mathscr{C}_1 \rightarrow \mathscr{C}_2)}{r(\mathscr{C}_1)} \dots\\ &\frac{W(\mathscr{C}_{K-1} \rightarrow \mathscr{C}_K)}{r(\mathscr{C}_{K-1})}f(\mathscr{C}_K, \tau_K)\delta(\tau_0 + \tau_1 + \dots + \tau_K - t_{\text{obs}})
    \end{aligned}
\end{equation}
But simplifying everything, we can obtain easily as
\begin{equation}
    \mathscr{P}[\text{traj}] = P_{\text{init}}(\mathscr{C}_0) e^{-\sum_{j=0}^{K}r(\mathscr{C_j})\tau_j}\prod_{j=0}^{K-1}\text{d}\tau_j \delta(t_{\text{obs}} - \sum_j \tau_j) \prod_{j=0}^{K-1}W(\mathscr{C}_j \rightarrow \mathscr{C}_{j+1})
\end{equation}

For a given trajectory "traj" we can also consier the time reversed one traj$^R$: 
\begin{equation}
    \mathscr{P}[\text{traj}^R] = P_{\text{final}}(\mathscr{C}_K) e^{-\sum_{j=0}^{K}r(\mathscr{C_j})\tau_j}\prod_{j=0}^{K-1}\text{d}\tau_j \delta(t_{\text{obs}} - \sum_j \tau_j) \prod_{j=0}^{K-1}W(\mathscr{C}_{j+1} \rightarrow \mathscr{C}_{j})
\end{equation}
From these probabilities, we define 
\begin{equation}
    \bar{Q}[\text{traj}] = \ln \left(\frac{\mathscr{P}[\text{traj}]}{\mathscr{P}[\text{traj}^R]}\right)
\end{equation}

We want to show an intriguing property of $P(\bar{Q}, t) = \text{Prob}\{\bar{Q}[\text{traj}] = \bar{Q}\}$:
\begin{equation}
    P(\bar{Q}, t) = \sum_{\text{traj}}\delta (\bar{Q} - \bar{Q}[\text{traj}])\mathscr{P}[\text{traj}]
\end{equation}
Noticing $\mathscr{P}[\text{traj}] = \mathscr{P}[\text{traj}^R]e^{\bar{Q}[\text{traj}]}$ and $\bar{Q}[\text{traj}] = -\bar{Q}[\text{traj}^R]$:

\begin{equation}
    P(\bar{Q}, t) = \sum_{\text{traj}^R} \delta(\bar{Q} + \bar{Q}[\text{traj}]) e^{-\bar{Q}[\text{traj}^R]}\mathscr{P}[\text{traj}^R]
\end{equation}
such that 
\begin{equation}
    P(\bar{Q}, t) = e^{\bar{Q}}P(-\bar{Q}, t)
\end{equation}
(Eraus-Searles theorem, 1993)\newline 
This leads to $<e^{-\bar{Q}}> = 1$. We see that 
\begin{equation}
    <\bar{Q}> = \sum_{\text{traj}} \bar{Q}[\text{traj}]\mathscr{P}[\text{traj}] = \sum_{\text{traj}} \mathscr{P}[\text{traj}] \ln\left(\frac{\mathscr{P}[\text{traj}]}{\mathscr{P}[\text{traj}]}\right)
\end{equation}
$\bar{Q}$ is somewhat a measure of how similar are the forward and backward processes. It is the Kullback-Lertler divergence/entropy. $<\bar{Q}>$ somewhat quantifies the asymmetry of the arrow of time. \par \medskip 
Reminder: $D(p || q) = \sum_i p_i \ln(\frac{p_i}{q_i}) \geq 0$ with equality iff $p_i = q_i$ \par \bigskip 

When we look at $\bar{Q}$, we see two contributions. 
\begin{equation}
    \bar{Q} = \ln\left(\frac{P_{\text{init}}(\mathscr{C}_0)}{P_{\text{final}}(\mathscr{C}_K)}\right) + \sum_{j=0}^{K-1} \ln\left(\frac{W(\mathscr{C}_j \rightarrow \mathscr{C}_{j+1})}{W(\mathscr{C}_{j+1} \rightarrow \mathscr{C}_j)}\right)
\end{equation}
The first term is a boundary term, whilst the second one (named $Q_s$) is time extensive since there are $K$ terms, and 
\begin{equation}
    \frac{\text{d}<K>}{\text{d}t} = <r(\mathscr{C})>
\end{equation}

We see that $Q_s$ does not contain any information on the $\tau_j$'s, it only depend on the sequence of visited states. We thus call it a history instead of a trajectory.

\subsection{Evolution of the Shannon entropy}
The Shannon entropy over the configs $\mathscr{C}$ is 
\begin{equation}
    S(t) = -\sum_\mathscr{C} P(\mathscr{C}, t) \ln(\mathscr{C}, t)
\end{equation}
Wether it makes sense or not, we can always, always define and consider it. This quantity evolves according to (after simplifications)
\begin{equation}
    \begin{aligned}
        \frac{\text{d}S}{\text{d}t} = &\frac{1}{2}\sum_{\mathscr{C}\mathscr{C}'} \left[W(\mathscr{C} \rightarrow \mathscr{C}')P(\mathscr{C}, t) - W(\mathscr{C}' \rightarrow \mathscr{C})P(\mathscr{C}', t)\right] \times \ln \frac{W(\mathscr{C} \rightarrow \mathscr{C}')P(\mathscr{C}, t)}{W(\mathscr{C}' \rightarrow \mathscr{C})P(\mathscr{C}', t)} \\ 
        &- \sum_{\mathscr{C}\mathscr{C}'} W(\mathscr{C} \rightarrow \mathscr{C}') \ln \frac{W(\mathscr{C} \rightarrow \mathscr{C}')}{W(\mathscr{C}' \rightarrow \mathscr{C})} P(\mathscr{C}, t)
    \end{aligned}
\end{equation}
The first term $\sigma_{\text{irr}}$ is positive, and vanishes iff 
\begin{equation}
    W(\mathscr{C} \rightarrow \mathscr{C}')P(\mathscr{C}, t) = W(\mathscr{C}' \rightarrow \mathscr{C})P(\mathscr{C}', t)
\end{equation}
When $P(\mathscr{C}, t)$ becomes stationnary, this property is verified by $P_{ss}$, promotes $P_{ss}$ to the status of an "equilibrium definition"
\begin{equation}
    P_{\text{eq}}(\mathscr{C})W(\mathscr{C} \rightarrow \mathscr{C}') = P_{\text{eq}}(\mathscr{C}') W(\mathscr{C}' \rightarrow \mathscr{C})\text{ :  detailed balance}
\end{equation}
The second term, $\frac{\text{d}S}{\text{d}t} = \sigma_{\text{irr}} - J_s$
\begin{equation}
    \begin{aligned}
        J_s &= \sum_{\mathscr{C}\mathscr{C}'} P(\mathscr{C}, t) W(\mathscr{C} \rightarrow \mathscr{C}') \ln \frac{W(\mathscr{C} \rightarrow \mathscr{C}')}{W(\mathscr{C}' \rightarrow \mathscr{C})} \\ 
        &= <\frac{\text{d}Q_s}{\text{d}t}>
    \end{aligned}
\end{equation}

In the steady-state, $\frac{\text{d}S}{\text{d}t} = 0$, 
\begin{equation}
    \sigma_{\text{irr}} = J_s \geq 0
\end{equation}
with equality iff detailed balance is fullfilled by $P_{ss}$.

\subsection{One dimensional walkers on a lattice}

Let's begin with $L$ sites with periodic boundary conditions, such that a particle hops to the left (i -= 1) with probability $p$ and to the right with probability $q$, and such that there is always maximum 1 particle per site. \par \medskip 

Given $N$ particles and $L$ sites, $P(\{n_i\}, t)$. What is the steady-state? Is it equilibrium? \par \medskip 

We can guess the steady state is a uniform distribution on each sites $P(\{n_i\}) = \frac{1}{\binom{L}{N}}$, and it is at equilibrium iff $p = q$. 

Let's make sure our guess is good: 
\begin{equation}
    \begin{aligned}
        \partial_t P(\mathscr{C}= \{n_i\}, t) &= p \sum_i n_i(1- n_{i+1}) P(\dots) \\ 
        &+ q \sum_i n_i (1 - n_{i-1}) P(\dots) \\ 
        &- p \sum_i n_i (1-n_i) P(\{n_i\}, t) - q \sum_i n_i (1 - n_{i+1})P(\{n_i\}, t)
    \end{aligned}
\end{equation}
The two first terms account for particles coming there coming from elsewhere, and the two last account for particles leaving. And indeed, $P_{ss}(\{n_i\}) = \text{cst}$ solves it in the steady-state. \par \medskip 

How about $\bar{Q}_s[\text{traj}]$?
\begin{equation}
    \begin{aligned}
        \bar{Q}_s[\text{traj}] &= \sum_{i=0}^{K-1}\ln \frac{W(\mathscr{C}_j \rightarrow \mathscr{C}_{j+1})}{W(\mathscr{C}_{j+1} \rightarrow \mathscr{C}_j)} \\ 
        &= \left(\ln \frac{q}{p}\right) \times (\text{nb of hops to the right}) + \left(\ln \frac{p}{q}\right) \times (\text{nb of hops to the left}) \\ 
        &= \ln \frac{q}{p} \times \left(\text{Total integrated particle current}\right)
    \end{aligned}
\end{equation} 
$\ln \frac{q}{p}$ is the driving force of the current, or the affinity in term of chemistry. We thus understand the $\bar{Q}$ as an entropy creation, as a force times a current. \par \medskip 

If we parametrize $p = D_0e^{-E/2}$, $q=D_0e^{E/2}$ then 
\begin{equation}
    \frac{\text{d}<Q_s>}{\text{d}t} = J_s = E\times <\text{particle current}>
\end{equation}

\section{First passage properties and adjoint master equation}
\subsection{Backward master equation}

We are interested in $P(\mathscr{C}, t | \mathscr{C}', t')$ = the probability to be in state $\mathscr{C}$ at time $t$, starting from $\mathscr{C}'$ at time $t'$. 
\begin{equation}
    P(\mathscr{C}, t | \mathscr{C}', t) = \delta_{\mathscr{C}\mathscr{C}'}
\end{equation}
\begin{equation}
    \partial_t P(\mathscr{C}, t | \mathscr{C}', t') = \sum_{\mathscr{C}''} W_{\mathscr{C}\mathscr{C}''} P(\mathscr{C}'', t | \mathscr{C}', t')
\end{equation}
\begin{equation}
    P(\mathscr{C}, t | \mathscr{C}', t') = \left[e^{W(t-t')}\right]_{\mathscr{C}\mathscr{C}'}
\end{equation}
And it's possible to track the evolution 
\begin{equation}
    \begin{aligned}
        \partial_{t'}P(\mathscr{C}, t | \mathscr{C}', t') &= \sum_{\mathscr{C}''} P(\mathscr{C}, t | \mathscr{C}'', t')W_{\mathscr{C}''\mathscr{C}'} \\ 
        &= - (W^\dagger)_{\mathscr{C}'\mathscr{C}''}P(\mathscr{C}, t | \mathscr{C}'', t')
    \end{aligned}
\end{equation}
Hence $W^\dagger$ propagates backward in time. 

\subsection{First passage probability and first passage time}

Let $\mathcal{A}$ be a set of absorbing configurations. We ask about the survival probability $S(\mathscr{C}, t)$ that starting from $\mathscr{C}$ such that $\mathscr{C} \notin \mathcal{A}$ the system is still alive at time $t$. In practice, 
\begin{equation}
    S(\mathscr{C}, t) = \sum_{\mathscr{C}'' \notin \mathcal{A}} P(\mathscr{C}'', t | \mathscr{C}, 0)
\end{equation}
with boundary conditions 
\begin{equation}
    \forall \mathscr{C} \in \mathcal{A}, S(\mathscr{C}, t) = 0, \qquad \forall \mathscr{C} \notin \mathcal{A}, S(\mathscr{C}, 0) = 1
\end{equation}\par \medskip 

Using the master equation, we see that 
\begin{equation}
    \begin{aligned}
        \partial_t S(\mathscr{C}, t) &= \sum_{\mathscr{C}''} W_{\mathscr{C}''\mathscr{C}} S(\mathscr{C}'', t) \\
        \partial_t S &= W^\dagger S
    \end{aligned}
\end{equation}

We have the set of equations 
\begin{equation}
    \begin{aligned}
        &S(\mathscr{C}, t) = 1 - \int_{0}^{t} \text{d}t' F_\mathcal{A}(\mathscr{C}, t') \\ 
        & -\frac{\text{d}S}{\text{d}t} = F_\mathcal{A}
    \end{aligned}
\end{equation}

We are interested in $T_\mathcal{A}(\mathscr{C})$ = average time of first passage to $\mathcal{A}$ starting from $\mathscr{C}$. 
\begin{equation}
    T_\mathcal{A} = \int_{0}^{+\infty} \text{d}t ~ t~ F_\mathcal{A}(\mathscr{C}, t)
\end{equation}
Because after an integration by part, 
\begin{equation}
    T_\mathcal{A}(\mathscr{C}) = \int_{0}^{+\infty} \text{d}t ~ S(\mathscr{C}, t)
\end{equation}
then we can show that 
\begin{equation}
    W^\dagger T_\mathcal{A} = -1
\end{equation}
or 
\begin{equation}
    (W^\dagger)_{\mathscr{C}\mathscr{C}'}T_\mathcal{A}(\mathscr{C}') = -1
\end{equation}



\chapter{Stochastic dynamics}

\section{What is the question?}

We start with a micro system, very very hard to describe. We consider a big colloid of the order of the $\mu m$ described by $(\vec{R}, \vec{P})$ in a bath of small molecules of water $i$ of size $\sim 3.4$\r{A} described by $(\vec{r}_i, \vec{p}_i)$. We can accurately write this system in term of a large Hamiltonian 
\begin{equation}
    \mathscr{H} = \mathscr{H}_0(\vec{R}, \vec{P}) + \mathscr{H}_1((\vec{r}_i, \vec{p}_i)_i) + \sum_i V_i(\vec{r}_i, \vec{p}_i, \vec{R}, \vec{P})
\end{equation}
where the first term describes the dynamics of the colloid, the second term describes the dynamics of the water molecules, and the last term describes the interactions between the two. Moreover, we know that at a higher level of description, if the system is static, we have 
\begin{equation}
    P(\vec{R}, \vec{P}) = \frac{1}{Z} e^{-\beta \mathscr{H}(\vec{R}, \vec{P})}
\end{equation}
where there is no information left about the bath whatsoever, no trivial information. This is in general the objective of statistical physics, discarding useless information. \par \medskip 
Now, the question is wether or not it is possible to obtain an equation similar to the Boltzmann distribution, but describing the time evolution of the colloid. To do so, the idea is to split the molecules of water surrounding the colloid into multiple small regions, in packages sufficiently big to forget about the interactions between the packages, but sufficiently small to have a large number of them around the colloid. 
\begin{remark}
    This is only possible because there are several orders of difference between the water molecules and the colloid. The correlation length of water molecules is about 10\r{A}, so we can do packages of size 100\r{A} and still have around a hundred packages envelopping the colloid. The following thinking couldn't be possible if there was no such difference in magnitude.
\end{remark}

When such division into packages is possible, we can assert that the strength put by the water molecules onto the colloid is equal to 
\begin{equation}
    \begin{aligned}
        \vec{F}_b = \sum_i \vec{F}_i &= \sum_{\text{region}} \sum_{i \in \text{region}} \vec{F}_i  \\
        &\simeq <\vec{F}_b> + \text{gaussian fluctuation}
    \end{aligned}
\end{equation}
where $<\vec{F}_b>$ corresponds to the visquous friction, and is an average at fixed $(\vec{R}, \vec{P})$ over all possible $(\vec{r}_i, \vec{p}_i)$. \par \medskip 

In view of this, we expect that after coarse-graining, we get something like 
\begin{equation}
    M\frac{\text{d}^2\vec{R}}{\text{d} t^2} = -\frac{\partial \mathscr{H}_0}{\partial\vec{R}} + <\vec{F}_b> + \text{noise}
\end{equation}
where the first term is due to the physics of the colloid itself, the second one corresponds to the drag, and the last term corresponds to the random action of the water particles on the body. 

\section{Master equation, again}
\subsection{For one and several continuous variables}

We now denote a config $\mathscr{C}$ by a $d$-dimensional vector $x$. We will often assume $d=1$, as the generalisation can be done easily. The master equation writes
\begin{equation}
    \partial_t P(x, t) = \int \text{d}x' w(x' \rightarrow x) P(x', t) - \int \text{d}x' w(x \rightarrow x') P(x, t)
\end{equation}
We choose to write $w$ as a function of the length $w(x \rightarrow x') = w(x, x' - x)$.
\begin{equation}
    \partial_t P(x, t) = \int \text{d}r w(x-r, r) P(x-r, t) - \int \text{d}r w(x, r) P(x, t)
\end{equation}
We expand $w$ for low $r$
\begin{equation}
    \partial_t P(x, t) = \sum_{n\geq 1} \frac{(-1)^n}{n!} \partial^n_x \left[\int \text{d}r~ r^n w(x, r) P(x, t)\right]
\end{equation}
So defining 
\begin{equation}
    a_n(x) = \int \text{d}r ~ r^n w(x, r)
\end{equation}
The master equation rewrites 
\begin{equation}
    \partial_t P =  \sum_{n\geq 1} \frac{(-1)^n}{n!} \partial^n_x (a_n(x)P(x, t))
\end{equation}
So as expected from a conserved current, we have 
\begin{equation}
    \partial_t P = -\partial_\mu J^\mu
\end{equation}
Where 
\begin{equation}
    J^\mu = \text{ probability current}
\end{equation}
which is defined up to a quantity. There is a name for $a_n$: it is the Kramers-Moyal coefficient of order $n$. 

\subsection{Infinitesimal jumps}
Assume that at $t=t_0$, the system is at $x(t_0) = x_0$. Then rum over the dynamics over $[t_0, t_0 + \Delta t]$. Them ask about the random variable 
\begin{equation}
    \Delta x = x(t_0 + \Delta t) - x(t_0)
\end{equation}
when $\Delta t$ is small. We certainly want to solve 
\begin{equation}
    \partial_t P = -\partial_x J
\end{equation}
with $p(x, t | x_0, t_0)$ verifying 
\begin{equation}
    p(x, t_0 | x_0, t_0) = \delta(x - x_0)
\end{equation}
\begin{equation}
    <\Delta x^k> = \int \text{d}x (x-x_0)^k p(x, t_0 + \Delta t | x_0, t_0)
\end{equation}
where 
\begin{equation}
    p(x, t_0 + \Delta t | x_0, t_0) = p(x, t_0 | x_0, t_0) + \Delta t \partial_t p + O(\Delta t^2)
\end{equation}
So $\forall k \geq 1$
\begin{equation}
    <\Delta x^k> = \Delta t \sum \frac{1}{n!}\int \text{d}x \frac{\text{d}^n}{\text{d}x^n} (x-x_0)^k \delta(x - x_0)a(x_0)
\end{equation}
Which gives 
\begin{equation}
    \frac{<\Delta x^k>}{\Delta t} =_{\Delta t \rightarrow 0} a_k(x_0)
\end{equation}
So $a_k(x_0)$ is the $k^{\text{th}}$ momentum of the displacement over $[t_0, t_0 + \Delta t]$ 

\subsection{Approximation}
If we think of the example of the colloid, we expect $\Delta x$ to be small with respect to the scale over which the colloid evolves. \par \medskip 

As a first idea, we can truncate 
\begin{equation}
    \partial_t p = -\partial_x J = -partial_x a_1(x)p(x, t)
\end{equation}
because 
\begin{equation}
    a_n(x) = \int \text{d}r ~ r^n w(x, r)
\end{equation}
is small. At this level of truncation we end up on a deterministic propagation equation in which the probability cloud moves as a block. If we truncate at order 2:
\begin{equation}
    \partial_t  p = -\partial_x (a_1(x)p) + \frac{1}{2}\partial^2(a_2(x)p)
\end{equation}
This is a diffusion equation: Fokker-Planck equation. In high dimension, we see a diffusion equation on a curved spacetime, where the metric of the spacetime is encoded in $a_2$
\begin{equation}
    \partial_t  p = -\partial_\mu (a_1^\mu p) + \frac{1}{2}\partial_\mu\partial_\nu(a_2^{\mu\nu} p)
\end{equation}
However, we cannot expand at higher order: if we truncate at order $k\geq 3$, the positivity of $p$ is not ensured. This is very important, since $p$ is a probability. Hence, we have no choice but to keep our expansion (very limited) at order 1 or 2, or to keep the whole equation.

\section{Langevin's equation}
\subsection{Gaussian random variables}

A Gaussian random variable $\xi$ has a probability density 
\begin{equation}
    P(\vec{\xi}) = \frac{1}{Z} e^{-\frac{1}{2}\xi^\dagger \Gamma \xi}
\end{equation}
with $\Gamma$ a positive definite symmetric matrix, and 
\begin{equation}
    Z = (2\pi)^{M/2}/\sqrt{\det \Gamma}
\end{equation}
where $M$ is the dimension of $\vec{\xi}$. 
With this we can define 
\begin{equation}
    G[h] = <e^{h^\dagger \xi }> = e^{\frac{1}{2}h^\dagger \Gamma^{-1}h}
\end{equation}

We know picture a Gaussian variable as a discrete time process in which $m=1,\dots, M$ is viewed as a time slice index: $t_m = m\Delta t$. Eventually, we want to take the $\Delta t \rightarrow 0$ limit, or $M \rightarrow +\infty$, while keeping $t_\text{obs} = M\Delta t = \text{fixed}$. Hence, we write 
\begin{equation}
    \begin{aligned}
        \text{d}^M\xi P(\xi) &= Z^{-1} \text{d}\xi^1\dots \xi_M e^{-\frac{1}{2} \sum_{mn} \xi_m \Gamma_{mn}\xi_n} \\
        &=  Z^{-1} \text{d}\xi^1\dots \xi_M e^{-\frac{1}{2}\int \text{d}t\text{d}t' \xi(t)\Gamma(t, t')\xi(t')}
    \end{aligned}
\end{equation} 
with $t = m\Delta t$, $\xi(t) = \frac{1}{\sqrt{\Delta t}}\xi_t$, $\Gamma(t, t') = \frac{1}{\Delta t}\Gamma_{tt'}$. \par \medskip 

Usually, one writes 
\begin{equation}
    \frac{1}{Z}\text{d}\xi^1\dots \xi_M = \mathcal{D}\xi
\end{equation}
The matrix $\Gamma(t, t')$ is called a noise kernel, and if $G(t, t')$ (Green's propagator) is its inverse:
\begin{equation}
    \int \text{d}t'' \Gamma(t, t'')G(t'', t') = \delta (t-t')
\end{equation}
where we have introduced 
\begin{equation}
    G(t, t') = \frac{G_{t/\Delta t, t'/\Delta t}}{\Delta t} = <\xi(t), \xi(t')>
\end{equation}
If the $\Delta t \rightarrow 0$ limit exists when computing averagesand correlations of physical quantities, $\xi(t)$ is called a Gaussian process. \par \medskip 

One exemple that will come back very often is one for which 
\begin{equation}
    \Gamma(t, t') = G(t, t') = \delta(t-t')
\end{equation}
In this case, $\xi$ is a Gaussian white noise. For instance, if we are interested in 
\begin{equation}
    x_M = \Delta t^{1/2} \sum_{m=1}^M \xi_m 
\end{equation}
then for $\Gamma_{nm} = \delta_{nm}$ we have 
\begin{equation}
    \begin{aligned}
        &<x_M> = 0 \\
        &<x^2_M> = M\Delta t = t_{\text{obs}}
    \end{aligned}
\end{equation}
This can be recovered with the continuum limit 
\begin{equation}
    x(t_{\text{obs}}) = \int_{0}^{t_{\text{obs}}}\text{d}s \xi(s)
\end{equation}

\subsection{Stochastic Differential Equation}
Let $x_0$ be fixed, and define $x_n$ via the following recursion relation 
\begin{equation}
    x_{n+1} = x_n + f(x_n)\Delta t + g(x_n)\sqrt{\Delta t} \eta_n
\end{equation}
with $f, g$ given functions, and $\eta_n$ a Gaussian process such that $<\eta_n\eta_m> = \delta_{nm}$. \par \medskip 

In the continuum limit, this reads, with $x(t) = x_{t/\Delta t}$
\begin{equation}
    \Delta x = x(t + \Delta t) - x(t) = f(x(t))\Delta t + g(x(t))\sqrt{\Delta t}\eta_m 
\end{equation}
where 
\begin{equation}
    \sqrt{\Delta t}\eta_m  = \Delta\eta = \int_{t}^{t + \Delta t}\text{d}s~ \eta(s)
\end{equation}
with 
\begin{equation}
    \eta(s) = \frac{\eta_{s/\Delta t}}{\sqrt{\Delta t}}
\end{equation}
$\Delta \eta$ is Gaussian with variance $\Delta t$
\begin{equation}
    \begin{aligned}
        &<\Delta \eta> = 0 \\
        &<\Delta \eta ^2> = \int_{t}^{t+\Delta t} \text{d}s\text{d}s' <\eta(s)\eta(s')> = \Delta t
    \end{aligned}
\end{equation}
It is tempting and natural to write 
\begin{equation}
    \frac{\text{d}x}{\text{d}t} = f(x(t)) + g(x(t))\eta(t)
\end{equation}
We immediatly see that it is NOT fine to assume $x$ is differentiable: 
\begin{equation}
    \frac{\Delta x}{\Delta t} \sim O(\frac{1}{\sqrt{\Delta t}})
\end{equation}
it explodes. The continuum limit of 
\begin{equation}
    \Delta x = f(x(t)) \Delta t + g(x(t)) \Delta \eta \Rightarrow \dot{x} = f + g\eta
\end{equation}
is full of traps. Suppose we are given $\alpha \in [0,1]$, then 
\begin{equation}
    \Delta x = f(x(t) + \alpha \Delta x) \Delta t + g(x(t) + \alpha \Delta x)\Delta \eta
\end{equation}
then it looks like as $\Delta t \rightarrow 0$, $\dot{x} = f + g\eta$. So given an equation of the form $\dot{x} = f + g\eta$, we do not have a way of a priori knowing what is the intended discretized form, so we make it explicit by dressing the $=$ sign with $\alpha$

\subsection{Infinitesimal jumps}
Let's focus on the statistics of the infinitesimal jumps $\Delta x = x(t_0 + \Delta t) - x_0$, with $x(t_0) = x_0 = \text{fixed}$. We assume that 
\begin{equation}
    \begin{aligned}
        &\Delta x = f(x(t_0))\Delta t + g(x(t_0))\Delta \eta \\
        &\Rightarrow <\Delta x> = f(x(t_0)) \Delta t \Rightarrow \lim_{\Delta t \rightarrow 0} \frac{<\Delta x>}{\Delta t} = f(x_0) \\ 
        &\Rightarrow \lim_{\Delta t \rightarrow 0} \frac{<\Delta x^2>}{\Delta t} = g^2(x_0)
    \end{aligned}
\end{equation}
Moreover, we can compute that for any $k\geq 3$, we have $ \lim_{\Delta t \rightarrow 0} \frac{<\Delta x^k>}{\Delta t} = 0$ \par \medskip

We know that the statistical properties of $x(t)$ are identical to those of a process governed by the following master's equation 
\begin{equation}
    \partial_t p = -\partial_x (a_1 (x)p ) + \frac{1}{2}\partial^2_x (a_2(x)p)
\end{equation}
in which we have chosen 
\begin{equation}
    a_1(x) = f(x) \qquad a_2(x) = g^2(x)
\end{equation}

\subsection{Stochastic calculus}

Since there is a variety of discretized proesses that lead to the same (visually speaking) continuous SDE (stochastic differential equation), we'd like to understand better the connections between these various discretizations. We shall now take $0 \leq \alpha \leq 1$, and consider 
\begin{equation}
    \begin{aligned}
        &\Delta x = x(t + \Delta t) - x(t) = f(x(t) + \alpha\Delta t) \Delta t + g(x(t) + \alpha \Delta x)\Delta \eta \\
        &\Leftrightarrow_{\Delta t \rightarrow 0} \dot{x} =^\alpha f + g \eta
    \end{aligned}  
\end{equation}
With 
\begin{equation}
    \Delta \eta = \int_{t}^{t+\Delta t}\text{d}s~ \eta(s)
\end{equation}
Our goal is to find, if it exists, the corresponding Fokker-Planck equation. 
\begin{equation}
    \lim_{\Delta t \rightarrow 0} \frac{<\Delta x^k>}{\Delta t} = a_k 
\end{equation}
\begin{equation}
    \Delta x = x(t_0 + \Delta t ) - x(t_0)
\end{equation}
\begin{equation}
    \begin{aligned}
        \Rightarrow ~~&<\Delta x> = <f(x_0 + \alpha \Delta x)\Delta t + g(x_0 + \alpha \Delta x)\Delta \eta>\\ 
        &= <f(x_0) \Delta t + \alpha \Delta x \Delta t f'(x_0) + \frac{1}{2} \alpha^2 \Delta x^2f''(x_0)\Delta t + \dots > \\ 
        &+ <[g(x_0) + \alpha \Delta x g'(x_0) + \dots]\Delta \eta> \\
        & = f(x_0)\Delta t + g(x_0)<\Delta \eta> + \alpha g'(x_0)<\Delta x \Delta \eta> + O(\Delta t ^{3/2}) 
    \end{aligned}
\end{equation}
We also compute
\begin{equation}
    <\Delta x \Delta \eta> = g(x_0) <\Delta \eta^2> + O(\Delta t^{3/2})
\end{equation}
knowing
\begin{equation}
    <\Delta \eta> = 0, \qquad <\Delta \eta^2> = \Delta t
\end{equation}
We get 
\begin{equation}
    \lim_{\Delta t \rightarrow 0} \frac{<\Delta x>}{\Delta t} = f(x_0) + \alpha g'(x_0)g(x_0)
\end{equation}
We carry out the same reasonning for $\Delta x^2$:
\begin{equation}
    \lim_{\Delta t \rightarrow 0}\frac{<\Delta x^2>}{\Delta t} = g^2(x_0)
\end{equation}
And for all $k \geq 3$, 
\begin{equation}
    \lim_{\Delta t \rightarrow 0}\frac{<\Delta x^k>}{\Delta t} = 0
\end{equation}


Hence when we write 
\begin{equation}
    \dot{x} =^\alpha f + g\eta
\end{equation}
We see that it's fully equivalent to 
\begin{equation}
    \dot{x} =^0 f + \alpha g'g + g\eta
\end{equation}
Even if both are visually different, they encode the same physics. Both are described by the same Fokker-Planck equation 
\begin{equation}
    \partial_t \rho = -\partial_x [(f(x) + \alpha g'(x)g(x))\rho] + \frac{1}{2}\partial_x^2 (g^2(x)\rho)
\end{equation}
When $g$ is a constant,
\begin{equation}
    \begin{aligned}
        \dot{x} &=^\alpha  f + g\eta \\
        &=^0 f + g\eta
    \end{aligned}
\end{equation}
This is an additive (noise) Langevin equation, but contrast to g(x) not a constant, which leads to a multiplicative Langevin equation. \par \bigskip 

Let $u(t)$ be a random process slaved to $x(t)$:
\begin{equation}
    u(t) = U (x(t))
\end{equation}
Our goal: find a SDE for $u$ knowing $\dot{x} =^\alpha f + g\eta$. We begin with 
\begin{equation}
    \begin{aligned}
        \Delta u &= u(t_0 + \Delta t) - u(t_0) = U(x_0 + \Delta x) - U(x_0) \\
        &= \Delta x U'(x_0) + \frac{1}{2} U''(x_0)\Delta x^2 + O(\Delta t^{3/2}) \\
        &= [f(x_0 + \alpha \Delta x)\Delta t + g(x_0 + \alpha \Delta x)\Delta \eta]U'(x_0) \\
        &~~~+ \frac{1}{2}U''(x_0)[f(x_0 + \alpha \Delta x)\Delta t + g(x_0 + \alpha \Delta x)\Delta \eta]^2 + O(\Delta t^{3/2})
    \end{aligned}
\end{equation}

Such that 
\begin{equation}
    \begin{aligned}
        \Delta u = &f(x_0)\Delta t U'(x_0) \\
        &+g(x_0) U'(x_0)\Delta \eta \\
        &+\alpha g'(x_0)U'(x_0)\Delta x\Delta \eta \\
        &+ \frac{1}{2} U''(x_0)g^2(x_0)\Delta \eta ^2 \\
        &+ O(\Delta t ^{3/2})
    \end{aligned}
\end{equation}
So
\begin{equation}
    \lim_{\Delta t \rightarrow 0}\frac{<\Delta u>}{\Delta t} = f(x_0)U'(x_0) + \alpha g'gU' + \frac{1}{2}U'' g^2
\end{equation}
\begin{equation}
    \lim_{\Delta t \rightarrow 0}\frac{<\Delta u^2>}{\Delta t} = U'(x_0)^2\Delta x^2 + O(t^{3/2})
\end{equation}
And for all $k \geq 3$, 
\begin{equation}
    \lim_{\Delta t \rightarrow 0}\frac{<\Delta u^k>}{\Delta t} = 0
\end{equation}
We could now write a Fokker-Planck equation for 
\begin{equation}
    q(u, t) = \text{Prob}\{ u \leq u(t) \leq u+\text{d}u\}/\text{d}u
\end{equation}
\begin{equation}
    \partial_t q = -\frac{\partial}{\partial u}\left([(f + \alpha g'g)U + \frac{1}{2}g^2U'']q\right) + \frac{1}{2}\partial_u^2(g^2U^{'2} q)
\end{equation}
where $x$ has to be expressed in term of $u$. \par \medskip 
Hence, if $\dot{x}=^\alpha f + g\eta$ then 
\begin{equation}
    \frac{\text{d}u}{\text{d}t} =^0 (f + \alpha g'g)U' + \frac{1}{2}g^2 U'' + gU'\eta
\end{equation}
If we had started from $\alpha = 0$
\begin{equation}
    \dot{x} =^0 f + g\eta
\end{equation}
then 
\begin{equation}
    \dot{u} =^0 \dot{x}U' + \frac{g^2U''}{2}
\end{equation}
It$\bar{o}$'s lemma. But if $\alpha \neq 0$, $\dot{x}=^\alpha f + g\eta$ leads to 
\begin{equation}
    \begin{aligned}
        \dot{u} &=^0 (f + \alpha g'g)U' + \frac{1}{2}g^2 U'' + gU'\eta \\
        &=^{\alpha'} (f + \alpha g'g)U' + \frac{1}{2}g^2 U'' + gU'\eta - \alpha' \frac{\text{d}}{\text{d}u}\left(gU'\right)gU'\\
        &=^{\alpha'} (f + (\alpha-\alpha') g'g)U' + (\frac{1}{2}-\alpha)g^2 U'' + gU'\eta 
    \end{aligned}
\end{equation}
\begin{equation}
    \dot{u} =^{\alpha} \dot{x}U' + (\frac{1}{2}- \alpha)g^2U''
\end{equation}

On this version of the It$\bar{o}$'s lemma, we see explicitely that the chain rule fails. But we also see that $\alpha = \frac{1}{2}$ plays a particular role where formally, the chain rule holds 
\begin{equation}
    \begin{aligned}
        &\dot{x} =^{\frac{1}{2}} f + g\eta \\ 
        &\dot{u} =^{\frac{1}{2}} \dot{x}U'
    \end{aligned}
\end{equation}

"Choices" of $\alpha$ : 
\begin{itemize}
    \item $\alpha = 0$: (prepoint) Ito discretization, very nice for computers, but we lose differential calculus (no chain rule)
    \begin{equation}
        x_{n+1} = x_n + f(x_n)\Delta t + g(x_n)\sqrt{\Delta t}\eta_n, \quad <\eta_n\eta_m> = \delta_{n,m}
    \end{equation}
    \item $\alpha = \frac{1}{2}$: (midpoint) Statonovich discretization: not cool, implicit recursion, almost impossible to implement on a computer. But cool, we have chain rule. Even cooler, physics consistent. It is also the only one consistent with time reversal
    \item $\alpha = 1$: (postpoint) Klimontovich-Hanggi, nice for plasma physics. We want to write everything in path integral formalism, and use Matsubara time, which uses post point discretization
\end{itemize}

Of course, all of this extends to 
\begin{equation}
    \dot{x}^\mu =^\alpha f^\mu + g^{\mu i}\eta_i
\end{equation}
with $\mu = 1,\dots, d$ and $i = 1, \dots, d' \leq d$. $u = U^\mu(x)$ evolves according to 
\begin{equation}
    \dot{u}^\mu =^0 \partial_\nu U^\mu f^\nu + \partial_\nu U^\mu g^{\nu i}\eta_i + \frac{1}{2}\partial_\beta \partial_\gamma U^\mu \omega^{\beta\gamma}
\end{equation}
with $\omega^{\beta\gamma} = g^{\beta i}g^{\gamma i}$. 

\section{Connection to physics}
What is the connection between $\dot{x} = f + g\eta$ and physics? Let's go back to the colloid in water picture. 
\begin{equation}
    M\frac{\text{d}\vec{P}}{\text{d}t} = \vec{F}_b + \vec{F}_{\text{ext}}
\end{equation}

We separate the bath into regions 
\begin{equation}
    \begin{aligned}
        \vec{F}_b &= \sum \vec{F}_i, \quad \vec{F}_i = \text{ force created by region }i \\
        &= <\vec{F}_b> + \text{ Gaussian fluctuations}
    \end{aligned}
\end{equation}
where $<\vec{F}_b>$ is an average over the baths at fixed $\vec{R}, \vec{P}$. \par \medskip 
We try to write something as general as possible, at the linear level
\begin{equation}
    <\vec{F}_b> = -\int \text{d}t'~ M_R(t-t')\frac{\text{d}\vec{R}}{\text{d}t'}
\end{equation}
Where $M_R(t-t')$ can depend functionally on $\vec{R}$. Condensing all of the above assumptions, we can guess
\begin{equation}
    M\frac{\text{d}^2 \vec{R}}{\text{d}t^2} = \vec{F}_{\text{ext}} - \int \text{d}t'~ M_R(t-t')\frac{\text{d}\vec{R}}{\text{d}t'} + \vec{xi}
\end{equation} 
where $M_R(t-t')$ is the response kernel, the response of the environment to the movement of the colloid. On the other hand, we have 
\begin{equation}
    <\xi^\mu(t)\xi^\nu(t)> = TM^{\mu\nu}_c (t-t')
\end{equation}
with $M^{\mu\nu}_c (t-t')$ the correlation kernel. \par \medskip 

There has to be a strong connection between $M_R$ and $M_c$ because they are two faces of the same physical process. If the physics of the system is such that the memory of the bath is quickly forgotten, then it is fine to replace 
\begin{equation}
    M_R(t-t') = \gamma \delta(t-t')
\end{equation}
and 
\begin{equation}
    M_c(t-t') = \gamma' \delta(t-t')
\end{equation}
So that 
\begin{equation}
    M \frac{\text{d}^2 \vec{R}}{\text{d}t^2} = \vec{F}_{\text{ext}} - \gamma \frac{\text{d}\vec{R}}{\text{d}t} + \sqrt{2\gamma' T}\vec{\eta}
\end{equation}
\begin{equation}
    <\eta^\mu(t)\eta^\nu(t)> = \sigma^{\mu\nu}\delta(t-t')
\end{equation}
This is an underdamped Langevin equation. \par \medskip 

For a colloid in water, the overdamped limit is when we can make the approximation that there is an almost instantaneous balance between $\vec{F}_{\text{ext}}$ and $\vec{F}_b$, so that inertia can be neglected
\begin{equation}
    \vec{0} \simeq \vec{F}_{\text{ext}} + \vec{F}_b
\end{equation}

\begin{equation}
    \begin{aligned}
        &-\gamma \frac{\text{d}\vec{R}}{\text{d}t} + \sqrt{2\gamma'T} \eta + \vec{F}_{\text{ext}} = 0 \\
        &\frac{\text{d}\vec{R}}{\text{d}t} = \mu \vec{F}_{\text{ext}} + \sqrt{2\gamma' \mu^2}\vec{\eta}
    \end{aligned}
\end{equation}
with $\mu = \frac{1}{\gamma}$ the mobility.  \par \medskip 

We'll take this sort of equations as a starting point. For instance, for an isolated particle, in the underdamped description
\begin{equation}
    d=1: \qquad  M\frac{\text{d}v}{\text{d}t} = -\gamma v + \sqrt{2\gamma' T }\eta
\end{equation}
where the Markov approximation $M_R \propto \delta(t)$ was used. But we want to derive an evolution equation for 
\begin{equation}
    K = \frac{Mv^2}{2}
\end{equation}
From the one above, assuming we are using a $\alpha = 1/2$ discretization, we get 
\begin{equation}
    \frac{\text{d}K}{\text{d}t} =^{1/2} M v\dot{v} = -\gamma Mv^2 + Mv \sqrt{2\gamma'T}\eta
\end{equation}
or assuming we were using a $\alpha = 0$ discretization, we get 
\begin{equation}
    \frac{\text{d}K}{\text{d}t} =^0 \dot{v}\frac{\text{d}K}{\text{d}v} + \frac{1}{2}\frac{\text{d}^2 K}{\text{d}v^2}2\gamma'T 
\end{equation}
which leads through computations to 
\begin{equation}
    \frac{M<v^2>}{2} = \frac{\gamma'}{\gamma}\frac{T}{2}
\end{equation}
from which we understand that we must have $\gamma' = \gamma$ \par \medskip 

Multiplicative noise also occurs directly for a simple overdamped colloid in water 
\begin{equation}
    \gamma \frac{\text{d}\vec{R}}{\text{d}t} = \sqrt{2\gamma' T}\vec{\eta}
\end{equation}
In a river, $\gamma$ depends on the distance to the wall. 

\section{Path integrals for stochastic processes}
\subsection{Starting from a Langevin equation}
We begin with $\dot{x} =^\alpha f+g\xi$. 

\begin{equation}
    \Delta x_m = x_{m+1} - x_m = f(x_m + \alpha \Delta x_m) \Delta t + g(x_m + \alpha \Delta x_m)\sqrt{\Delta t}~\xi_m
\label{eq:pilangevin}\end{equation}
with $x_0$ given, $m = 0, \dots, M-1$, and $\xi_0, \dots, \xi_{M-1}$ independant Gaussian variables 
\begin{equation}
    <\xi_i, \xi_j> = \delta_{ij}
\end{equation}
Goal: write 
\begin{equation}
    <A(x(t_{\text{obs}}))> = \int \mathcal{D}_{\text{path}} e^{-\mathcal{S}[\text{path}]}A(\text{path})    
\end{equation} 
We have 
\begin{equation}
    <A(x_m)> = \int \frac{\text{d}\xi_0}{\sqrt{2\pi}}\dots\frac{\text{d}\xi_{M-1}}{\sqrt{2\pi}} e^{-\frac{\xi_0^2}{2}-\dots-\frac{\xi_{M-1}^2}{2} } A(x_M(\xi_0, \dots, \xi_{M-1}))
\end{equation}
where $x_m$ is the solution of \eqref{eq:pilangevin} at a given set $\xi_0, \dots, \xi_{M-1}$.
We perform a change of integration variable 
\begin{equation}
    (\xi_0, \dots, \xi_{M-1}) \rightarrow (x_1,\dots , x_M)
\end{equation}
We have 
\begin{equation}
    \text{d}\xi_0 \dots \text{d}\xi_{M-1} = \text{d}x_1 \dots \text{d}x_m \left|  \text{det}\left(\frac{\partial\xi_i}{\partial x_j}\right)\right|
\end{equation}
where 
\begin{equation}
    \xi_i = \frac{\Delta x_i - f_i \Delta t}{g_i \sqrt{\Delta t}}
\end{equation}
We can compute the determinant by using only the diagonal, since we may notice that the matrix is triangular. 
\begin{equation}
    \begin{aligned}
        \frac{\partial \xi_i}{\partial x_i+1} &= \frac{1 - \alpha f'_i\Delta t}{g_i \sqrt{\Delta t}} - \frac{\alpha g'_i}{g_i}\frac{\Delta x_i - f_i \Delta t}{g_i \sqrt{\Delta t}} \\
        &= \frac{1}{g_i \sqrt{\Delta t}}[1 - \alpha f'_i \Delta t - \frac{\alpha g'_i}{g_i}(\Delta x_i - f_i \Delta t)]
    \end{aligned}
\end{equation}
Then 
\begin{equation}
    <A(x_M)> = (2\pi)^{-M/2} \int \text{d}x_1 \dots \text{d}x_M \prod_{i=0}^{M-1}\frac{1}{g_i \sqrt{\Delta t}}[1 - \alpha f'_i \Delta t - \frac{\alpha g'_i}{g_i}(\Delta x_i - f_i \Delta t)] A(x_M )e^{\dots}
\end{equation}
Then using some kind of Fourier transform (denoting by a $\bar{x}$ a variable being integrated over the imaginary line), we get 
\begin{equation}
    <A(x_M)> = (2\pi)^{-M}\int \text{d}x_1 \dots \text{d}x_M \text{d}\bar{x}_0 \dots \text{d}\bar{x}_{M-1} A(x_M)\prod_{i=0}^{M-1}\frac{1}{g_i \sqrt{\Delta t}}[1 - \alpha f'_i \Delta t - \frac{\alpha g'_i}{g_i}(\Delta x_i - f_i \Delta t)] e^{-\sum_{i=0}^{M-1}\bar{x}_i[\Delta x_i - f_i \Delta t] - \frac{g_i^2}{2}\bar{x_i}^2\Delta t}
\end{equation}
But now, we can replace $\Delta x_i - f_i \Delta t$ to get 
\begin{equation}
    <A(x_M)> = (2\pi)^{-M}\int \text{d}x_1 \dots \text{d}x_M \text{d}\bar{x}_0 \dots \text{d}\bar{x}_{M-1} A(x_M)\prod_{i=0}^{M-1}[1 - \alpha f'_i \Delta t - \alpha g'_ig_i\bar{x}_i\Delta t] e^{-\sum_{i=0}^{M-1}\bar{x}_i[\Delta x_i - f_i \Delta t] - \frac{g_i^2}{2}\bar{x_i}^2\Delta t}
\end{equation}

Such that we can write for small $\Delta t$
\begin{equation}
    <A(x_M)> = (2\pi)^{-M}\int \text{d}x_1 \dots \text{d}x_M \text{d}\bar{x}_0 \dots \text{d}\bar{x}_{M-1} A(x_M) e^{-\mathcal{S}[\{\bar{x}_i, x_j\}]}
\end{equation}
with 
\begin{equation}
    \mathcal{S} = \sum_i \left[\bar{x}_i (\Delta x_i - f_i \Delta t) - \frac{g_i^2}{2}\bar{x}_i^2 \Delta t + \alpha f'_i \Delta t + \alpha g'_i g_i \bar{x}_i \Delta t\right]
\end{equation}
Or in continuous variables, 
\begin{equation}
    <A(x(t_{\text{obs}}))> = \int  \mathcal{D}\bar{x} \mathcal{D}x A(x(t_{\text{obs}}))e^{-\mathcal{S}[\bar{x}, x]}
\end{equation}
with
\begin{equation}
    \mathcal{S} =^\alpha \int_{0}^{t_\text{obs}} \text{d}t [\bar{x}(\dot{x} - f) - \frac{g^2}{2}\bar{x}^2 + \alpha f' + \alpha g'g\bar{x}]
\end{equation}

\subsection{The Janssen - De Dominicis action}

In the continuum, $\dot{x} =^\alpha f+g\xi$ is equivalent to 
\begin{equation}
    <A> = \int \mathcal{D}\bar{x}\mathcal{D}x A e^{-\mathcal{S}}
\end{equation}
with $\mathcal{S}[\bar{x}, x] =^\alpha \int_{0}^{t_\text{obs}} \text{d}t [\bar{x}(\dot{x} - f) - \frac{g^2}{2}\bar{x}^2 + \alpha f' + \alpha g'g\bar{x}]$. "Martini Sigga Rose". We see that if we had used $\alpha = 0$, things would have been simpler. It turns out that the Ito form is the simplest to use for perturbation theory 
\begin{equation}
    \begin{aligned}
        &\dot{x} =^\alpha f + g\xi \\ 
        &\dot{x} =^0 f + \alpha gg' + g\xi \\
        &\mathcal{S} =^0 \int [\bar{x}(\dot{x} - f - \alpha g'g) - \frac{g^2}{2}\bar{x}^2]
    \end{aligned}
\end{equation}
One can show that 
\begin{equation}
    <\bar{x}(t)x(t)> = \Theta(0) = \alpha
\end{equation}
and thus $\alpha = 0$ is more convenient for explicit perturbation expansions. 

\subsection{The dirty way for Ito}

Let's be bold and write 
\begin{equation}
    \dot{x} =^0 f + g\eta 
\end{equation}
solution of $x[\eta]$ and compute 
\begin{equation}
    <A(x(t_{\text{obs}}))> = \int \mathcal{D}\eta ~e^{-\int \frac{\eta^2}{2}} A(x[\eta](t_{\text{obs}})) \times \int \mathcal{D}x \int \mathcal{D}\bar{x} e^{-\bar{x}(\dot{x} - f- g\eta)}
\end{equation}
integrating $\eta$ out, 
\begin{equation}
    <A> = \int \mathcal{D}\bar{x}\mathcal{D}x A(x) e^{-\mathcal{S}}
\end{equation}
with 
\begin{equation}
    \mathcal{S} = \int \bar{x}(\dot{x} - f) - \frac{g^2}{2}\bar{x}^2
\end{equation}

\subsection{Onsager-Machlup functorial}
When $g$ is a constant, the $\bar{x}$ field is not necessarily needed, and we can expess averages in terms of the physical field only 
\begin{equation}
    \mathcal{S} =^\alpha \int \frac{(\dot{x} - f)^2}{2g^2} + \alpha f'
\end{equation}
This is known as the Onsager-Machlup functional. 

\section{Quantum Langevin equation}

\subsection{Some questions}A
At the classical level, we derived that 
\begin{equation}
    m\dot{\dot{x}} = -V'(x) - \gamma \dot{x} + \sqrt{2\gamma T}\eta
\end{equation}
How to quantize this? We start with a quantum system with Hamiltonian 
\begin{equation}
    \mathcal{H} = \mathcal{H}_0(\vec{R}, \vec{P}) + \mathcal{H}_b 
\end{equation}
and interactions between the bath and the particles. We introduce some simplifying assumptions. In quantum mechanics, 
\begin{equation}
    \frac{\text{d}\vec{R}}{\text{d}t} = \frac{i}{\hbar}[\mathcal{H}, vec{R}] = \vec{P}/M
\end{equation}
assuming $\mathcal{H}_0 = \frac{vec{P}}{2M} + V(\vec{R})$. We also have 
\begin{equation}
    \frac{\text{d}\vec{P}}{\text{d}t} = \frac{i}{\hbar}[\mathcal{H}, \vec{P}] = -\frac{\partial V}{\partial \vec{R}} - \sum_i k_i(\vec{R} - \vec{r}_i)
\end{equation}
with 
\begin{equation}
    \frac{\text{d}\vec{r}_i}{\text{d}t} = \vec{p}_i/m_i
\end{equation}
etc etc etc we continue with computations see the notes, at the end we find 
\begin{equation}
    <\xi(t)\xi(t') + xi(t')xi(t)> = \sum_j \frac{\hbar}{2m_j \omega_j} \text{coth}(\frac{\beta \hbar \omega_j}{2})\cos (\omega_j(t-t'))
\end{equation}
If we want to say more, we have to explain what the $k_i, m_i, \omega_i$'s are
\chapter{Time-reversal}

\section{Defining equilibrium}
\subsection{Statistical mechanics with trajectories}
We return to a description in terms of a master equation 
\begin{equation}
    \partial_t P(C, t) = \sum_{C'} W(C' \rightarrow C)P(C', t) - r(C)P(C, t)
\end{equation}
and we have seen that 
\begin{equation}
    P[\text{traj}] = P_0(C_0) \delta(t_{\text{obs}} - \sum_{j=0}^k \tau_j) e^{-\sum_j r(C_j)\tau_j}\prod_j W(C_j \rightarrow C_{j+1})
\end{equation}

is the probability density to observe a time realization
\begin{equation}
    C_0 \rightarrow^{\tau_0} C_1 \rightarrow \dots \rightarrow C_K \rightarrow^{\tau_K} C_K
\end{equation}
and 
\begin{equation}
    \bar{Q}[\text{traj}] = \ln(\frac{P[\text{traj}]}{P[\text{traj}^R]})
\end{equation}
\begin{equation}
    Q_S[\text{traj}] = \sum_j \ln{\frac{W(C_j \rightarrow C_{j+1})}{W(C_{j+1} \rightarrow C_j)}}
\end{equation}
If we introduce 
\begin{equation}
    S(t) = -\sum_C P(C, t) \ln P(C, t)
\end{equation}
then 
\begin{equation}
    \frac{\text{d}S}{\text{d}t} = \sigma_{\text{irr}} - J_S, \quad \sigma_{\text{irr}} \geq 0, \quad J_s = \frac{\text{d}}{\text{d}t}<Q_s>
\end{equation}

In a stationnary (steady) state, $\frac{\text{d}S}{\text{d}t} = 0$ hence 
\begin{equation}
    \sigma_{\text{irr}} = J_S \geq 0
\end{equation}
The steady-state is an equilibrium if and only if 
\begin{equation}
    \sigma_{\text{irr}} = J_S = 0
\end{equation}
ie 
\begin{equation}
    W(C \rightarrow C') P(C) = W(C' \rightarrow C) P(C') = 
\end{equation}

\subsection{The words and world of stochastic energetics}
Let's introduce fluctuating observables
\begin{equation}
    \hat{S}(t) = -\ln(P(C(t), t))
\end{equation}
By construction, 
\begin{equation}
    <\hat{S}> = S
\end{equation}
Over a small time interval $[t, t+\Delta t]$
\begin{equation}
    \hat{S}(t + \Delta t) - \hat{S}(t) = -\ln \frac{P(C(t + \Delta t), t)}{P(C(t), t)} - \Delta t \frac{\partial_t P}{P}
\end{equation}
and averaging leads us back to $\frac{\text{d}S}{\text{d}t} = \sigma_{\text{irr}} - J_s$. 

Equivalently, we can get a whole microscopic fluctuating picture for thermodynamics, see the notes for more details. For the second principle, we get 
\begin{equation}
    \bar{Q} = \Delta \hat{S} + Q_s
\end{equation}
Where 
\begin{equation}
    <\bar{Q}> = D_{KL}(P[\text{traj}]|P[\text{traj}^R]) \geq 0
\end{equation}
Equilibrium is defined as a steady state in which $<\bar{Q}> = 0$. This is equivalent to stating 
\begin{equation}
    <\bar{Q}> = <Q_s> = 0
\end{equation}

\subsection{Application to Langevin dynamics}

In principle all proofs apply to 
\begin{equation}
    m\frac{\text{d}^2\vec{x}}{\text{d}t^2} = -\int M_R(t-t') \frac{\text{d}\vec{x}}{\text{d}t'} + \vec{F}_{\text{ext}} + \vec{\eta}
\end{equation}
with
\begin{equation}
    <\eta^\alpha (t) \eta^\beta(t')> = TM_C(t - t') \delta^{\alpha\beta}
\end{equation}
But for simplicity we restrict ourselves to 
\begin{itemize}
    \item The overdamped limit 
    \item The Markov limit $M_R, M_C \propto \delta$
\end{itemize}
Such that this simplifies to 
\begin{equation}
    \gamma \dot{\vec{x}} = \vec{F}_{\text{ext}} + \sqrt{2T\gamma'}\vec{\xi}
\end{equation}
with 
\begin{equation}
    <\xi^\alpha (t) \xi^\beta(t')> = \delta(t - t') \delta^{\alpha\beta}
\end{equation}
Let's write $P[\text{traj}]$ over $[0, t_{\text{obs}}]$. 
\begin{equation}
    P[\vec{x}] = P_{\text{init}}(\vec{x}(0)) e^{-\frac{1}{4T\gamma'}\int_0^{t_{\text{obs}}}\text{d}t (\gamma \dot{\vec{x}} - \vec{F}_{\text{ext}})^2 - \frac{1}{2\gamma}\int_{0}^{t_{\text{obs}}}\text{d}t \vec{\nabla}\cdot \vec{F}_{\text{ext}}}
\end{equation}
The time reversed trajectory is 
\begin{equation}
    \vec{x}^R (t) = \vec{x}(t_{\text{obs}} - t)
\end{equation}
such that 
\begin{equation}
    P[\vec{x}^R] = P_{f}(\vec{x}(t_\text{obs})) e^{-\frac{1}{4T\gamma'}\int_0^{t_{\text{obs}}}\text{d}t (-\gamma \dot{\vec{x}} - \vec{F}_{\text{ext}})^2 - \frac{1}{2\gamma}\int_{0}^{t_{\text{obs}}}\text{d}t \vec{\nabla}\cdot \vec{F}_{\text{ext}}}
\end{equation}
We get 
\begin{equation}
    \begin{aligned}
        \bar{Q}[\vec{x}] &= \ln\frac{P_{\text{init}}(\vec{x}(0))}{P_{f}(\vec{x}(t_\text{obs}))} -\frac{1}{4T\gamma'}\int_0^{t_{\text{obs}}}\text{d}t (\gamma \dot{\vec{x}} - \vec{F}_{\text{ext}})^2  - (-\gamma \dot{\vec{x}} - \vec{F}_{\text{ext}})^2 \\
        &= \ln\frac{P_{\text{init}}(\vec{x}(0))}{P_{f}(\vec{x}(t_\text{obs}))} + \frac{\gamma}{\gamma '}\frac{1}{T}\int_0^{t_{\text{obs}}} \text{d}t \dot{\vec{x}} \cdot \vec{F}_{\text{ext}}
    \end{aligned}
\end{equation}
If $\vec{F}_{\text{ext}} = -\vec{\nabla}V$ is conservative, then 
\begin{equation}
    \bar{Q} = \ln\frac{P_{\text{init}}(\vec{x}(0))}{P_{f}(\vec{x}(t_\text{obs}))} + \frac{\gamma}{\gamma '}\frac{1}{T}\left[V(\vec{x}(t_\text{obs})) - V(\vec{x}(0))\right]
\end{equation}
In a steady state, $P_i = P_f = P_ss$ and thus 
\begin{equation}
    \bar{Q} = \ln P_{ss}(\vec{x}(0))  + \frac{\gamma}{\gamma '}\frac{1}{T}V(\vec{x}(t_\text{obs})) - \ln P_{ss}(\vec{x}(t_\text{obs})) - \frac{\gamma}{\gamma '}\frac{1}{T}V(\vec{x}(0))
\end{equation}
And thus if there is equilibrium then 
\begin{equation}
    P_{ss}(\vec{x}) = \frac{e^{-\frac{V(\vec{x})}{T}}}{Z}
\end{equation}
and $\gamma = \gamma '$ is a property of the thermostat. \par \medskip 

Let's now use again $\gamma' = \gamma$, but $\vec{F}_{\text{ext}}$ not necessarily conservative 
\begin{equation}
    \bar{Q} = \left[\ln P_{ss}(\vec{x}(0)) - \ln P_{ss}(\vec{x}(t_{\text{obs}}))  \right] + \frac{1}{T} \int_{0}^{t_{\text{obs}}} \text{d}t ~\dot{\vec{x}}\cdot \vec{F}_{\text{ext}}
\end{equation}
where we recognize the first term to be $\Delta \hat{S}$ and the second one to be $Q_s$. Hence, $TQ_s$ is the work done by $\vec{F}_{\text{ext}}$ on the system over $[0, t_{\text{obs}}]$.\par 
But $-\gamma \dot{\vec{x}} + \sqrt{2\gamma T}\vec{\xi} + \vec{F}_{\text{ext}} = \vec{0}$, so writing $\vec{F}_b = -\gamma \dot{\vec{x}} + \sqrt{2\gamma T}\vec{\xi}$ the force of the bath on the system, we have 
\begin{equation}
    \vec{F}_{\text{ext}} = -\vec{F}_{\text{b}}
\end{equation}
The force of the system on the thermostat. We see
\begin{equation}
    Q_s = \frac{1}{T}\int_{0}^{t_{\text{obs}}} \text{d}t ~\dot{\vec{x}} \cdot (-\vec{F}_b)
\end{equation}
is equal to the heat given out by the thermostat divided by $T$. In a steady state, 
\begin{equation}
    J_s = <\dot{Q}_s> = \frac{1}{T}<\dot{\vec{x}}\cdot \vec{F}_{\text{ext}}> \geq 0
\end{equation}
\subsection{Consequences of equilibrium for the evolution operator}
At the master equation level, assuming equilibrium dynamics means that the rates $W$ statisfy 
\begin{equation}
    W(C\rightarrow C')P_{\text{eq}}(C) = W(C'\rightarrow C)P_{\text{eq}}(C')
\end{equation}
This has a direct consequence for $W$: 
\begin{equation}
    W_s = PWP^{-1}
\end{equation}
where $P_{CC'} = P_{\text{eq}}(C)\delta_{CC'}$
\begin{equation}
    \begin{aligned}
        W_{s~CC'} &= W(C' \Rightarrow C)\sqrt{\frac{P_{\text{eq}}(C')}{P_{\text{eq}}(C)}} \quad \text{if }C\neq C'\\
        &= -r(C) \quad \text{if }C=C'
    \end{aligned}
\end{equation}

\begin{equation}
    W_{s~CC'} = \sqrt{W(C\rightarrow C')W(C'\rightarrow C)} - r(C)\delta_{CC'}
\end{equation}
We see here that $W_s = W_s^\dagger$ is symmetric, so its spectrum, or the one of $W$, is real. There is a definite smell of quantum mechs. But 
\begin{itemize}
    \item There is no $i$
    \item state vector $\propto$ prob
\end{itemize} 
See Cardy, 96. \par \bigskip 

To be more concrete, let's have a look at at an overdamped Langevin equation. 
\begin{equation}
    \frac{\text{d}\vec{x}}{\text{d}t} = -\vec{\nabla} V + \sqrt{2T}\vec{\eta}, \quad \gamma = 1
\end{equation}
\begin{equation}
    Wp = \partial_t p = \vec{\nabla}\cdot (\vec{\nabla} Vp) + T\vec{\nabla}^2 p
\end{equation}
We have $W^\dagger \neq W$, but 
\begin{equation}
    W_s = T\vec{\nabla}^2 - \frac{1}{4T}(\vec{\nabla}V)^2 + \frac{1}{2}\Delta V 
\end{equation}
The evolution equation for 
\begin{equation}
    \tilde{p}(\vec{x}, t) = P_{eq}^{-1/2} p(x, t)
\end{equation}
reads 
\begin{equation}
    \begin{aligned}
        \partial_t \tilde{p} &= W_s \tilde{p} \\ 
        &= \text{nooo he erased}
    \end{aligned}
\end{equation}
Funnily, there is some physical meaning to the combination 
\begin{equation}
    r(\vec{x}) = \text{cst} + \frac{(\vec{\nabla}V)^2}{4T} - \frac{\Delta V}{2}
\end{equation}
is the rate at which the particle escapes from location $\vec{x}$, where the constant term diverges but represents the brownian motion, which has no knowledge of the energetic background and which eventually doesn't really matter. 

\section{Consequences of time reversibility}

\subsection{The fluctuation-dissipation theorem at the level of a master equation}

The starting point is a master equation with rates $W_0(C \rightarrow C')$ respecting detailed balance property with respect to $P_{eq}(C)$. 
\begin{equation}
    r_0(C) = \sum_{C'} W_0(C \rightarrow C') 
\end{equation}
is the escape rate in equilibrium. This defines the unperturbed dynamics. We now add a perturbation 
\begin{equation}
    W(C \rightarrow C') = W_0 (C \rightarrow C') e^{\frac{\beta}{2}h(t)[A(C) - A[C']]}
\end{equation}
where $h$ depends on time ($h$ small, $h(t<0) = 0$) and $A$ is some observable. If $h$ were a constant, we would have detailed balance with respect to $P_{eq}e^{\beta h A}$, $H = H_0 - hA$. So changing the rates this way just correspond to adding a small varying field, $hA$. \par \medskip 

The goal is to study 
\begin{equation}
    \frac{\delta<B(t)>_h}{\delta h(t')}\Bigg|_h = R(t, t')
\end{equation}
the response of $B$ at time $t$ for a perturbation of $A$ at time $t'< t$. \par \medskip 

We need to compute 
\begin{equation}
    <B>_h - <B>_{h=0}
\end{equation}
and identify $R$ such that 
\begin{equation}
    <B>_h^{(t)} - <B>_0^{(t)} = \int^t \text{d}t'~ R(t, t')h(t')
\end{equation}
The master equation reads for small $h$
\begin{equation}
    \partial_t P = (W_0 + \delta W)P
\end{equation}
with 
\begin{equation}
    \delta W_{CC'} = W_0(C'\rightarrow C)\frac{\beta h}{2} [A(C) - A[C']] -\delta r(c)\delta_{CC'}
\end{equation}
\begin{equation}
    \delta r(C) =\frac{\beta h}{2} \sum_{C'} W_0(C\rightarrow C')(A(C') - A[C])
\end{equation}
So
\begin{equation}
    P = P_{eq} + \delta P, \quad \partial_t \delta P = W_0 \delta P + \delta W P_{eq} 
\end{equation}
We want $\delta P$ because 
\begin{equation}
    \begin{aligned}
        \delta <B> &= <B>_h (t) - <B>_0 (t)\\
        &= \sum_C \delta P(C, t) B(C)
    \end{aligned}
\end{equation}
and we see that 
\begin{equation}
    \delta P_C = \int_{0}^{t}\text{d}t'~ (e^{W_0(t-t')})_{CC'} \delta W_{C'C''}P_{eq}(C'') 
\end{equation}
\begin{equation}
    \begin{aligned}
        \delta W_{C'C''} = -&\frac{\beta h}{2}W_{0~C'C''}A(C'') + \frac{\beta h}{2}W_0 (C'' \rightarrow C')A(C') \\
    &- \frac{\beta h}{2}\delta_{C'C''}\sum_{C'''} W_0 (C'\rightarrow C''')A(C''')
    \end{aligned}
\end{equation}
Such that 
\begin{equation}
    \begin{aligned}
        \delta <B> = -&\frac{\beta}{2} \sum_{CC'C''} \int_{0}^{t}\text{d}\tau ~ h(\tau) B(C)[e^{W_0(t - \tau)}]_{CC'}W_{0~C'C''} A(C'')P_{eq}(C'')   \\ 
    &- \frac{\beta}{2}\sum_{CC'C''} \int_{0}^{t}\text{d}\tau ~ B(\tau)[e^{W_0(t - \tau)}]_{CC'}W_{0~C'C''}\delta r(C', \tau) P_{eq}(C')
    \end{aligned}
\end{equation}
We have 
\begin{equation}
    R(t, t') = -\frac{\beta}{2} \frac{\text{d}}{\text{d}t}<B(t)A(t')> - \frac{\beta}{2}<B(t)\delta r(t')>
\end{equation}
"extended fluctuation -dissipation theorem". If we use detailed balance, the second piece simplifies into $-\frac{\beta}{2} \frac{\text{d}}{\text{d}t}<B(t)A(t')>$ so that 
\begin{equation}
    R(t, t') =-\beta \frac{\text{d}}{\text{d}t}<B(t)A(t')>
\end{equation}
this is the real fluctuation dissipation theorem, with 
\begin{equation}
    <B(t)A(0)> = \sum_{CC'}B(C)[e^{W_0(t - \tau)}]_{CC'}A(C') P_{eq}(C')
\end{equation}
There are several remarkable features. 
\begin{itemize}
    \item in equilibrium there is time translation invariance, ie $$C_{BA}(t, t') = <B(t)A(t')> = C_{BA}(t-t') = C_{BA}(t'-t)$$
    \item $R$ is also time translation invariant: $R(t, t') = R(t-t')$
\end{itemize}
\begin{equation}
    R(\tau) - R(-\tau) = -\beta \frac{\text{d}C}{\text{d}\tau}
\end{equation}
The FDT (fluctuation dissipation theorem) expresses that the relaxation rate of an equilibrium fluctuation is identical to the response of the system to a small external perturbation. \par \medskip 
The FDT $R = -\beta C$ depends neither on $A$ nor on $B$, and there exists a universal number 
\begin{equation}
    T = \frac{1}{\beta} = -\frac{\dot{C}}{R} = \text{temperature}
\end{equation}
There are generalizations to non-linear response. 

\subsection{The FDT at the level of a Langevin equation}
Let's start from 
\begin{equation}
    \dot{x} = -V' + f + \sqrt{2T}\eta 
\end{equation}
with $f$ a small external force at $t \geq 0$. We ask again about 
\begin{equation}
    R(t, t') = \frac{\delta <B(t)>}{\delta f(t')}\Bigg| _{f=0}
\end{equation}
We know that 
\begin{equation}
    <B> = \int \mathcal{D}x B(x(t)) e^{-\frac{1}{4T}\int (\dot{x} + V' - f)^2}
\end{equation}
we compute 
\begin{equation}
    R(t, t') = \frac{\delta<B(t)>}{\delta f(t')} = \frac{1}{2T}<B(x(t))(\dot{x}+V')(t')>
\end{equation}
and
\begin{equation}
    R(-t, -t') = \frac{1}{2T}<B(-t)(\dot{x} + V')(-t')>
\end{equation}
such that we can write
\begin{equation}
    R(t, t') - R(-t, -t') = -\frac{1}{T}\frac{\text{d}}{\text{d}t} C(t-t')
\end{equation}
with $C(t-t') = <B(t)x(t')>$
It turns out that if 
\begin{equation}
    \dot{x} = F + \sqrt{2T}\eta
\end{equation}
with $F$ not necessarily conservative, then one can apply a perturbation $f\rightarrow 0$, and then define $R$ and $C$ in the same way 
\begin{equation}
    \begin{aligned}
        J_s &= \frac{1}{T}<\dot{x} F> = \text{entropy production rate} \\ 
        &= \lim_{t \rightarrow 0} \frac{\text{d}}{\text{d}t} (TR(t) + \dot{C}(t))
    \end{aligned}
\end{equation}
Harada-Sasa, PRL 2005

\section{Fluctuation theorems}

\chapter{Metastability}

\chapter{Mean-field}

\chapter{Field theories}

\chapter{Exactly solvable models}

\end{document}