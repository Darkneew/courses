\documentclass[a4paper]{book}

% packages % 
\usepackage[utf8]{inputenc} 
\usepackage{fvextra}
\usepackage{csquotes}
\usepackage[french, italian, spanish, english]{babel}
\usepackage[T1]{fontenc}   
\usepackage{color}  
\usepackage{amsmath, dsfont, amssymb, amsthm, stmaryrd, mathrsfs}
\usepackage[style=alphabetic]{biblatex}
\usepackage{enumitem}
\usepackage[hidelinks]{hyperref}

% graphics %
\usepackage{graphicx}
\graphicspath{ {./images/} }

% environments %
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{example}{Example}



% bibliography %
\bibliography{bibliography} 

\begin{document}

% title %
\title{Advanced statistical physics and new applications}
\author{Buisine Léo\\Ecole Normale Superieure of Paris}
\maketitle

\tableofcontents

\chapter{Introduction}

Frédérid van Wijland: fvw@u-paris.fr \newline
studies statistical physics / field theory applied to stat phys, glass, active matter, \dots \par \medskip 

We don't believe it right now, but this course will be useful to us. \par \medskip 

We will have the notes allowed in the exam, but only hand written notes. \par \medskip 

The lecture notes are oversized, to help us find things we like, to feed us with interesting things. However, only what we see during the course is to be known in the exam. 

\chapter{Statistical dynamics}

We know stat mechs in equilibrium (stationnary and reversible?). We want to do stat mechs including time. \par \bigskip 

There is always a level of description of our system described by what is called the "Master equation". 

\section{The physics behind a master equation}

\subsection{Phase space and Liouville equation}
Imagine a physical system described by its phase space coordinates $\Gamma = \{q, p\}$, and its evolution is governed by Hamilton's equations of motion 
\begin{equation}
    q = \frac{\partial \mathscr{H}}{\partial p}, \quad p = - \frac{\partial \mathscr{H}}{\partial q}
\end{equation}
where $\mathscr{H}$ is the Hamiltonian encoding all dynamical informations of the system. \par \medskip 

Let $\rho(\Gamma, t)$ be the phase space density. Then we have a conservation equation in the phase space 
\begin{equation}
    \partial_t \rho + \partial_\Gamma \cdot (\rho \dot{\Gamma}) = 0
\end{equation}
By expanding, we obtain 
\begin{equation}
    \partial_t \rho = \{\mathscr{H}, \rho\}
\end{equation}
Because $\partial_\Gamma \cdot \dot\Gamma = 0$, the phase space flow is incompressible. This is the Liouville equation. \par \medskip 
Sometimes, we write it 
\begin{equation}
    \partial_t \rho = -i \mathscr{L} \rho
\end{equation}
as a linear operator, where $-i \mathscr{L}\cdot = \{\mathscr{H}, \cdot\}$

\subsection{Projection operator} 
If at the microscopic level some degrees of freedom $c$ have a distribution $\rho(c, t)$, then what happens to a "coarse-grained" quantity 
\begin{equation}
    C(c): \mathbb{P}(\mathscr{C}, t) = \text{Prob}\{C(c) = \mathscr{C} \text{ at time } t\}
\end{equation}
What we know is the evolution of $\rho(c, t)$:
\begin{equation}
    \partial_t \rho = w\rho = \sum_{c'}w_{cc'}\rho(c', t)
\end{equation}
with $w$ some linear operator. How can we find an evolution for $\mathbb{P}(\mathscr{C}, t)$? \par \medskip 

First, we can relate $P$ to $\rho$: 
\begin{equation}
    P(\mathscr{C}, t) = \sum_c \delta_{\mathscr{C}, C(c)}\rho(c, t)
\end{equation}
Idea: 
\begin{equation}
    \mathscr{P}f(c) = \frac{1}{\Omega(C(c))}\sum_{c'}\delta_{C(c), C(c')}f(c')
\end{equation}
where 
\begin{equation}
    \Omega(\mathscr{C}) = \sum_c \delta_{\mathscr{C}, C(c)}
\end{equation}
We can check that $\mathscr{P}^2 = \mathscr{P}$. 
Let $\bar{p}(c, t) = \mathscr{P}p(c, t)$, then one can access $P(\mathscr{C}, t)$ via 
\begin{equation}
    P(\mathscr{C}, t) = \sum_c \delta_{\mathscr{C}, C(c)}\bar{p}(c, t)
\end{equation}
Now we split $p = \bar{p} + q$ with $\bar{p} = \mathscr{P}$, $q = (1-\mathscr{P})p$. and we can start from $\partial_t p = wp$ and thus arrive at 
\begin{equation}
    \begin{aligned}
        &\partial_t \bar{p} = \mathscr{P} w(\bar{p} + q) \\ 
        &\partial_t q = (1- \mathscr{P})w(\bar{p} + q) \Rightarrow \partial_t q - (1-\mathscr{P})wq = (1-\mathscr{P})w\bar{p}
    \end{aligned}
\end{equation}
We solve the equation for $q$, and inject $q$ as a functional of $\bar{p}$ into 
\begin{equation}
    \partial_t\bar{p} = \mathscr{P}w\bar{p} + \mathscr{P}wq
\end{equation}
we get 
\begin{equation}
    \partial_t \bar{p} = \mathscr{P}w\bar{p} + \int_{0}^{t} \bar{d}t' ~ \mathscr{P}we^{(1-\mathscr{P})w(t-t')}(1-\mathscr{P})w\bar{p}(t')
\end{equation}
There is the emergence of a memory loss $e^{(1-\mathscr{P})w(t-t')}$. We can reconstruct an equation for $P(\mathscr{C}, t)$, which has the general form 
\begin{equation}
    \partial_t P = M^{(1)}P + \frac{0}{t}\text{d}t' M^{(2)}P(t')
\end{equation}
where $M^{(2)}$ encodes the memory of the degrees of freedom lost in the description 
\begin{equation}
    \text{"}M^{(2)}(t) \propto e^{(1-\mathscr{P})wt} \text{"}
\end{equation}
If the eigenvalues of $(1-\mathscr{P})wt$ are very large, this means that this memory is quickly lost 
\begin{equation}
    e^{-\frac{t}{\tau}} \sim_{\tau \sim 0} \tau \delta(t)
\end{equation}
and then the equation for $P$ reads 
\begin{equation}
    \partial_t P = M_{\text{eff}}P
\end{equation}

If the physical system of interest is such that its typical time scale are much larger than those characterizing the interactions with the environment, then we end up on a linear, 1st order in time, diff equation for $P$. This is a master equation. \par \medskip 

See lecture notes for more details 

\section{Master equation}

The best book talking of the master equation is the one by N. Van Kampen, stochastic processes... libgen

\subsection{Rates}

We will use the notation $\mathscr{C}$ to refer to "microscopic" configs. Let 
\begin{equation}
    W(\mathscr{C} \rightarrow \mathscr{C}', t)\text{d}t
\end{equation}
be the probability that the system hops from config $\mathscr{C}$ to config $\mathscr{C}'$ between $t$ and $t+\text{d}t$. Then the rate $r(\mathscr{C})$ at which the system escapes config $\mathscr{C}$ is 
\begin{equation}
    r(\mathscr{C}) = \sum_{\mathscr{C}'}W(\mathscr{C}\rightarrow\mathscr{C}')
\end{equation}

Then it is possible to write the evolution equation for $P(\mathscr{C}, t)$: 
\begin{equation}
    P(\mathscr{C}, t+\text{d}t) = P(\mathscr{C}, t)(1- r(\mathscr{C})\text{d}t) + \sum_{\mathscr{C}'}P(\mathscr{C}', t)W(\mathscr{C}'\rightarrow\mathscr{C})\text{d}t
\end{equation}
Implying 
\begin{equation}
    \partial_t P(\mathscr{C}, t) = \sum_{\mathscr{C}'}W(\mathscr{C}'\rightarrow\mathscr{C})P(\mathscr{C}', t) - r(\mathscr{C})P(\mathscr{C}, t)
\end{equation}
This expresses a random walk on a directed graph with vertices $\mathscr{C}$ and weights over edges given by the $W(\mathscr{C}, \mathscr{C}')$'s. 

It is sometimes convenient to introduce 
\begin{equation}
    \begin{aligned}
        &W(\mathscr{C}' \rightarrow \mathscr{C}) \quad \text{if } \mathscr{C} \neq \mathscr{C}' \\ 
        W_{\mathscr{C}\mathscr{C}'} =&\\
        &-r(\mathscr{C}) \quad \text{if }\mathscr{C} = \mathscr{C}
    \end{aligned}
\end{equation}
A posteriori, we see that, of course, there is a probability conservation 
\begin{equation}
    \frac{d}{dt}\sum_{\mathscr{C}}P(\mathscr{C}, t) = \sum_{\mathscr{C}\mathscr{C}'}W_{\mathscr{C}\mathscr{C}'} P(\mathscr{C}', t) = 0
\end{equation}
because 
\begin{equation}
    \forall \mathscr{C}', \quad \sum_\mathscr{C} W_{\mathscr{C}\mathscr{C}'} = 0 = \sum_{\mathscr{C}}\left[W(\mathscr{C}'\rightarrow \mathscr{C}) - r(\mathscr{C})\delta_{\mathscr{C}\mathscr{C}'}\right]
\end{equation}
Let $p_\mathscr{C} = 1$ for all $\mathscr{C}$, then 
\begin{equation}
    \sum_\mathscr{C} p_\mathscr{C} W_{\mathscr{C}\mathscr{C}'} = 0, \quad p^\dagger W = 0
\end{equation}

Hence the $p$ vector is a left eigenvector of $W$ with eigenvalue 0, hence there exists a right eigenvector $P_ss$ with eigenvalue 0. This is describing a steady-state. Let's restrict ourselves to strongly connected graphs of configurations (with irreducible dynamics), so as to avoid stationary probabilities being drained towards a specific subgraph. \par \medskip 

If we write 
\begin{equation}
    P(t + \text{d}t) = (1 + W\text{d}t)P(t)
\end{equation}
we apply the Perron-Frobenius theorem to $M = (1 + W\text{d}t)$, and we can conclude that there exists a positive  number $\rho$ that is an eigenvalue and all others are smaller. 
\begin{equation}
    l = \min_{\mathscr{C}} \sum_{\mathscr{C}'} M^\dagger_{\mathscr{C}\mathscr{C}'} \leq \rho \leq \max_{\mathscr{C}} \sum_{\mathscr{C}'}M^\dagger_{\mathscr{C}\mathscr{C}'} = 1
\end{equation}

Hence 0 is the largest eigenvalue of $W$, and the unique corresponding vector $P_ss$ has all its components of the same sign (we choose +).

\subsection{Averages}
In practice, given a master equation and a physical observable $B(\mathscr{C})$, it is possible to find the evolution of $<B>$:
\begin{equation}
    <B> = \sum_{\mathscr{C}} B(\mathscr{C})P(\mathscr{C}, t)
\end{equation}
such that 
\begin{equation}
    \begin{aligned}
        \frac{\text{d}<B>}{\text{d}t} &= \sum_{\mathscr{C}\mathscr{C}'} B(\mathscr{C})W_{\mathscr{C}\mathscr{C}'}P(\mathscr{C}', t) \\
        &= p^\dagger BWP \\ 
        &= p^\dagger [B, W]P
    \end{aligned}
\end{equation}
and a formal solution reads 
\begin{equation}
    <B(t)> = \sum_{\mathscr{C}\mathscr{C}'}B(\mathscr{C})(e^{Wt})_{\mathscr{C}\mathscr{C}'} P(\mathscr{C}', 0)
\end{equation}
Similarly, 
\begin{equation}
    <A(t_1)B(t_2)> = p^\dagger Ae^{W(t_1, t_2)}Be^{Wt_2}P_{\text{init}}
\end{equation}

\subsection{Trajectories and histories}

Let's see what the existence of rate actually mean: when the system is in state $\mathscr{C}$, then it stays there for a random duration before hopping to configuration $\mathscr{C}'$. \par \medskip 

Let $\Pi(\tau, \mathscr{C})\text{d}\tau$ be the probability to stay in state $\mathscr{C}$ for a duration $\tau$ and to hop to some other state between $\tau$ and $\tau + \text{d}\tau$
\begin{equation}
    \Pi(\tau, \mathscr{C}) = f(\mathscr{C}, \tau)r(\mathscr{C})\text{d}\tau
\end{equation}
where $f$ is the probability to stay a duration $\tau$ in state $\mathscr{C}$. 
\begin{equation}
    \begin{aligned}
        f(\mathscr{C},& \tau + \text{d}\tau) = f(\mathscr{C}, \tau)(1-r(\mathscr{C})\text{d}\tau) \\ 
        f(\mathscr{C},& 0) = 1 \\
        &\quad \Rightarrow f(\mathscr{C}, \tau) = e^{-r(\mathscr{C})\tau}
    \end{aligned}
\end{equation}

Let's look at a typical realization of the Markov process over some time window $[0, t_{\text{obv}}]$.
\begin{equation}
    \mathscr{C}_0 \rightarrow^{\tau_0} \mathscr{C}_1 \rightarrow^{\tau_1} \mathscr{C}_2 \rightarrow^{\tau_2} \dots \rightarrow^{\tau_{K-1}} \mathscr{C}_K\rightarrow^{\tau_{K}} \mathscr{C}_K
\end{equation}
with $K$ the number of hops over $[0, t_{\text{obs}}]$, which is a random variable. So $K$ is random, the $\tau_i$'s are random, and the $\mathscr{C}_i$'s are random. \par \medskip 

The probability of observing this trajectory is 
\begin{equation}
    \begin{aligned}
        \mathscr{P}[\text{traj}] = &P_{\text{init}}(\mathscr{C}_0) \Pi(\tau_0, \mathscr{C}_0) \text{d}\tau_0 \frac{W(\mathscr{C}_0 \rightarrow \mathscr{C}_1)}{r(\mathscr{C}_0)} \Pi(\tau_1, \mathscr{C}_1) \text{d}\tau_1 \frac{W(\mathscr{C}_1 \rightarrow \mathscr{C}_2)}{r(\mathscr{C}_1)} \dots\\ &\frac{W(\mathscr{C}_{K-1} \rightarrow \mathscr{C}_K)}{r(\mathscr{C}_{K-1})}f(\mathscr{C}_K, \tau_K)\delta(\tau_0 + \tau_1 + \dots + \tau_K - t_{\text{obs}})
    \end{aligned}
\end{equation}
But simplifying everything, we can obtain easily as
\begin{equation}
    \mathscr{P}[\text{traj}] = P_{\text{init}}(\mathscr{C}_0) e^{-\sum_{j=0}^{K}r(\mathscr{C_j})\tau_j}\prod_{j=0}^{K-1}\text{d}\tau_j \delta(t_{\text{obs}} - \sum_j \tau_j) \prod_{j=0}^{K-1}W(\mathscr{C}_j \rightarrow \mathscr{C}_{j+1})
\end{equation}

For a given trajectory "traj" we can also consier the time reversed one traj$^R$: 
\begin{equation}
    \mathscr{P}[\text{traj}^R] = P_{\text{final}}(\mathscr{C}_K) e^{-\sum_{j=0}^{K}r(\mathscr{C_j})\tau_j}\prod_{j=0}^{K-1}\text{d}\tau_j \delta(t_{\text{obs}} - \sum_j \tau_j) \prod_{j=0}^{K-1}W(\mathscr{C}_{j+1} \rightarrow \mathscr{C}_{j})
\end{equation}
From these probabilities, we define 
\begin{equation}
    \bar{Q}[\text{traj}] = \ln \left(\frac{\mathscr{P}[\text{traj}]}{\mathscr{P}[\text{traj}^R]}\right)
\end{equation}

We want to show an intriguing property of $P(\bar{Q}, t) = \text{Prob}\{\bar{Q}[\text{traj}] = \bar{Q}\}$:
\begin{equation}
    P(\bar{Q}, t) = \sum_{\text{traj}}\delta (\bar{Q} - \bar{Q}[\text{traj}])\mathscr{P}[\text{traj}]
\end{equation}
Noticing $\mathscr{P}[\text{traj}] = \mathscr{P}[\text{traj}^R]e^{\bar{Q}[\text{traj}]}$ and $\bar{Q}[\text{traj}] = -\bar{Q}[\text{traj}^R]$:

\begin{equation}
    P(\bar{Q}, t) = \sum_{\text{traj}^R} \delta(\bar{Q} + \bar{Q}[\text{traj}]) e^{-\bar{Q}[\text{traj}^R]}\mathscr{P}[\text{traj}^R]
\end{equation}
such that 
\begin{equation}
    P(\bar{Q}, t) = e^{\bar{Q}}P(-\bar{Q}, t)
\end{equation}
(Eraus-Searles theorem, 1993)\newline 
This leads to $<e^{-\bar{Q}}> = 1$. We see that 
\begin{equation}
    <\bar{Q}> = \sum_{\text{traj}} \bar{Q}[\text{traj}]\mathscr{P}[\text{traj}] = \sum_{\text{traj}} \mathscr{P}[\text{traj}] \ln\left(\frac{\mathscr{P}[\text{traj}]}{\mathscr{P}[\text{traj}]}\right)
\end{equation}
$\bar{Q}$ is somewhat a measure of how similar are the forward and backward processes. It is the Kullback-Lertler divergence/entropy. $<\bar{Q}>$ somewhat quantifies the asymmetry of the arrow of time. \par \medskip 
Reminder: $D(p || q) = \sum_i p_i \ln(\frac{p_i}{q_i}) \geq 0$ with equality iff $p_i = q_i$ \par \bigskip 

When we look at $\bar{Q}$, we see two contributions. 
\begin{equation}
    \bar{Q} = \ln\left(\frac{P_{\text{init}}(\mathscr{C}_0)}{P_{\text{final}}(\mathscr{C}_K)}\right) + \sum_{j=0}^{K-1} \ln\left(\frac{W(\mathscr{C}_j \rightarrow \mathscr{C}_{j+1})}{W(\mathscr{C}_{j+1} \rightarrow \mathscr{C}_j)}\right)
\end{equation}
The first term is a boundary term, whilst the second one (named $Q_s$) is time extensive since there are $K$ terms, and 
\begin{equation}
    \frac{\text{d}<K>}{\text{d}t} = <r(\mathscr{C})>
\end{equation}

We see that $Q_s$ does not contain any information on the $\tau_j$'s, it only depend on the sequence of visited states. We thus call it a history instead of a trajectory.

\subsection{Evolution of the Shannon entropy}
The Shannon entropy over the configs $\mathscr{C}$ is 
\begin{equation}
    S(t) = -\sum_\mathscr{C} P(\mathscr{C}, t) \ln(\mathscr{C}, t)
\end{equation}
Wether it makes sense or not, we can always, always define and consider it. This quantity evolves according to (after simplifications)
\begin{equation}
    \begin{aligned}
        \frac{\text{d}S}{\text{d}t} = &\frac{1}{2}\sum_{\mathscr{C}\mathscr{C}'} \left[W(\mathscr{C} \rightarrow \mathscr{C}')P(\mathscr{C}, t) - W(\mathscr{C}' \rightarrow \mathscr{C})P(\mathscr{C}', t)\right] \times \ln \frac{W(\mathscr{C} \rightarrow \mathscr{C}')P(\mathscr{C}, t)}{W(\mathscr{C}' \rightarrow \mathscr{C})P(\mathscr{C}', t)} \\ 
        &- \sum_{\mathscr{C}\mathscr{C}'} W(\mathscr{C} \rightarrow \mathscr{C}') \ln \frac{W(\mathscr{C} \rightarrow \mathscr{C}')}{W(\mathscr{C}' \rightarrow \mathscr{C})} P(\mathscr{C}, t)
    \end{aligned}
\end{equation}
The first term $\sigma_{\text{irr}}$ is positive, and vanishes iff 
\begin{equation}
    W(\mathscr{C} \rightarrow \mathscr{C}')P(\mathscr{C}, t) = W(\mathscr{C}' \rightarrow \mathscr{C})P(\mathscr{C}', t)
\end{equation}
When $P(\mathscr{C}, t)$ becomes stationnary, this property is verified by $P_{ss}$, promotes $P_{ss}$ to the status of an "equilibrium definition"
\begin{equation}
    P_{\text{eq}}(\mathscr{C})W(\mathscr{C} \rightarrow \mathscr{C}') = P_{\text{eq}}(\mathscr{C}') W(\mathscr{C}' \rightarrow \mathscr{C})\text{ :  detailed balance}
\end{equation}
The second term, $\frac{\text{d}S}{\text{d}t} = \sigma_{\text{irr}} - J_s$
\begin{equation}
    \begin{aligned}
        J_s &= \sum_{\mathscr{C}\mathscr{C}'} P(\mathscr{C}, t) W(\mathscr{C} \rightarrow \mathscr{C}') \ln \frac{W(\mathscr{C} \rightarrow \mathscr{C}')}{W(\mathscr{C}' \rightarrow \mathscr{C})} \\ 
        &= <\frac{\text{d}Q_s}{\text{d}t}>
    \end{aligned}
\end{equation}

In the steady-state, $\frac{\text{d}S}{\text{d}t} = 0$, 
\begin{equation}
    \sigma_{\text{irr}} = J_s \geq 0
\end{equation}
with equality iff detailed balance is fullfilled by $P_{ss}$.

\subsection{One dimensional walkers on a lattice}

Let's begin with $L$ sites with periodic boundary conditions, such that a particle hops to the left (i -= 1) with probability $p$ and to the right with probability $q$, and such that there is always maximum 1 particle per site. \par \medskip 

Given $N$ particles and $L$ sites, $P(\{n_i\}, t)$. What is the steady-state? Is it equilibrium? \par \medskip 

We can guess the steady state is a uniform distribution on each sites $P(\{n_i\}) = \frac{1}{\binom{L}{N}}$, and it is at equilibrium iff $p = q$. 

Let's make sure our guess is good: 
\begin{equation}
    \begin{aligned}
        \partial_t P(\mathscr{C}= \{n_i\}, t) &= p \sum_i n_i(1- n_{i+1}) P(\dots) \\ 
        &+ q \sum_i n_i (1 - n_{i-1}) P(\dots) \\ 
        &- p \sum_i n_i (1-n_i) P(\{n_i\}, t) - q \sum_i n_i (1 - n_{i+1})P(\{n_i\}, t)
    \end{aligned}
\end{equation}
The two first terms account for particles coming there coming from elsewhere, and the two last account for particles leaving. And indeed, $P_{ss}(\{n_i\}) = \text{cst}$ solves it in the steady-state. \par \medskip 

How about $\bar{Q}_s[\text{traj}]$?
\begin{equation}
    \begin{aligned}
        \bar{Q}_s[\text{traj}] &= \sum_{i=0}^{K-1}\ln \frac{W(\mathscr{C}_j \rightarrow \mathscr{C}_{j+1})}{W(\mathscr{C}_{j+1} \rightarrow \mathscr{C}_j)} \\ 
        &= \left(\ln \frac{q}{p}\right) \times (\text{nb of hops to the right}) + \left(\ln \frac{p}{q}\right) \times (\text{nb of hops to the left}) \\ 
        &= \ln \frac{q}{p} \times \left(\text{Total integrated particle current}\right)
    \end{aligned}
\end{equation} 
$\ln \frac{q}{p}$ is the driving force of the current, or the affinity in term of chemistry. We thus understand the $\bar{Q}$ as an entropy creation, as a force times a current. \par \medskip 

If we parametrize $p = D_0e^{-E/2}$, $q=D_0e^{E/2}$ then 
\begin{equation}
    \frac{\text{d}<Q_s>}{\text{d}t} = J_s = E\times <\text{particle current}>
\end{equation}

\section{First passage properties and adjoint master equation}
\subsection{Backward master equation}

We are interested in $P(\mathscr{C}, t | \mathscr{C}', t')$ = the probability to be in state $\mathscr{C}$ at time $t$, starting from $\mathscr{C}'$ at time $t'$. 
\begin{equation}
    P(\mathscr{C}, t | \mathscr{C}', t) = \delta_{\mathscr{C}\mathscr{C}'}
\end{equation}
\begin{equation}
    \partial_t P(\mathscr{C}, t | \mathscr{C}', t') = \sum_{\mathscr{C}''} W_{\mathscr{C}\mathscr{C}''} P(\mathscr{C}'', t | \mathscr{C}', t')
\end{equation}
\begin{equation}
    P(\mathscr{C}, t | \mathscr{C}', t') = \left[e^{W(t-t')}\right]_{\mathscr{C}\mathscr{C}'}
\end{equation}
And it's possible to track the evolution 
\begin{equation}
    \begin{aligned}
        \partial_{t'}P(\mathscr{C}, t | \mathscr{C}', t') &= \sum_{\mathscr{C}''} P(\mathscr{C}, t | \mathscr{C}'', t')W_{\mathscr{C}''\mathscr{C}'} \\ 
        &= - (W^\dagger)_{\mathscr{C}'\mathscr{C}''}P(\mathscr{C}, t | \mathscr{C}'', t')
    \end{aligned}
\end{equation}
Hence $W^\dagger$ propagates backward in time. 

\subsection{First passage probability and first passage time}

Let $\mathcal{A}$ be a set of absorbing configurations. We ask about the survival probability $S(\mathscr{C}, t)$ that starting from $\mathscr{C}$ such that $\mathscr{C} \notin \mathcal{A}$ the system is still alive at time $t$. In practice, 
\begin{equation}
    S(\mathscr{C}, t) = \sum_{\mathscr{C}'' \notin \mathcal{A}} P(\mathscr{C}'', t | \mathscr{C}, 0)
\end{equation}
with boundary conditions 
\begin{equation}
    \forall \mathscr{C} \in \mathcal{A}, S(\mathscr{C}, t) = 0, \qquad \forall \mathscr{C} \notin \mathcal{A}, S(\mathscr{C}, 0) = 1
\end{equation}\par \medskip 

Using the master equation, we see that 
\begin{equation}
    \begin{aligned}
        \partial_t S(\mathscr{C}, t) &= \sum_{\mathscr{C}''} W_{\mathscr{C}''\mathscr{C}} S(\mathscr{C}'', t) \\
        \partial_t S &= W^\dagger S
    \end{aligned}
\end{equation}

We have the set of equations 
\begin{equation}
    \begin{aligned}
        &S(\mathscr{C}, t) = 1 - \int_{0}^{t} \text{d}t' F_\mathcal{A}(\mathscr{C}, t') \\ 
        & -\frac{\text{d}S}{\text{d}t} = F_\mathcal{A}
    \end{aligned}
\end{equation}

We are interested in $T_\mathcal{A}(\mathscr{C})$ = average time of first passage to $\mathcal{A}$ starting from $\mathscr{C}$. 
\begin{equation}
    T_\mathcal{A} = \int_{0}^{+\infty} \text{d}t ~ t~ F_\mathcal{A}(\mathscr{C}, t)
\end{equation}
Because after an integration by part, 
\begin{equation}
    T_\mathcal{A}(\mathscr{C}) = \int_{0}^{+\infty} \text{d}t ~ S(\mathscr{C}, t)
\end{equation}
then we can show that 
\begin{equation}
    W^\dagger T_\mathcal{A} = -1
\end{equation}
or 
\begin{equation}
    (W^\dagger)_{\mathscr{C}\mathscr{C}'}T_\mathcal{A}(\mathscr{C}') = -1
\end{equation}



\chapter{Stochastic dynamics}

\section{What is the question?}

We start with a micro system, very very hard to describe. We consider a big colloid of the order of the $\mu m$ described by $(\vec{R}, \vec{P})$ in a bath of small molecules of water $i$ of size $\sim 3.4$\r{A} described by $(\vec{r}_i, \vec{p}_i)$. We can accurately write this system in term of a large Hamiltonian 
\begin{equation}
    \mathscr{H} = \mathscr{H}_0(\vec{R}, \vec{P}) + \mathscr{H}_1((\vec{r}_i, \vec{p}_i)_i) + \sum_i V_i(\vec{r}_i, \vec{p}_i, \vec{R}, \vec{P})
\end{equation}
where the first term describes the dynamics of the colloid, the second term describes the dynamics of the water molecules, and the last term describes the interactions between the two. Moreover, we know that at a higher level of description, if the system is static, we have 
\begin{equation}
    P(\vec{R}, \vec{P}) = \frac{1}{Z} e^{-\beta \mathscr{H}(\vec{R}, \vec{P})}
\end{equation}
where there is no information left about the bath whatsoever, no trivial information. This is in general the objective of statistical physics, discarding useless information. \par \medskip 
Now, the question is wether or not it is possible to obtain an equation similar to the Boltzmann distribution, but describing the time evolution of the colloid. To do so, the idea is to split the molecules of water surrounding the colloid into multiple small regions, in packages sufficiently big to forget about the interactions between the packages, but sufficiently small to have a large number of them around the colloid. 
\begin{remark}
    This is only possible because there are several orders of difference between the water molecules and the colloid. The correlation length of water molecules is about 10\r{A}, so we can do packages of size 100\r{A} and still have around a hundred packages envelopping the colloid. The following thinking couldn't be possible if there was no such difference in magnitude.
\end{remark}

When such division into packages is possible, we can assert that the strength put by the water molecules onto the colloid is equal to 
\begin{equation}
    \begin{aligned}
        \vec{F}_b = \sum_i \vec{F}_i &= \sum_{\text{region}} \sum_{i \in \text{region}} \vec{F}_i  \\
        &\simeq <\vec{F}_b> + \text{gaussian fluctuation}
    \end{aligned}
\end{equation}
where $<\vec{F}_b>$ corresponds to the visquous friction, and is an average at fixed $(\vec{R}, \vec{P})$ over all possible $(\vec{r}_i, \vec{p}_i)$. \par \medskip 

In view of this, we expect that after coarse-graining, we get something like 
\begin{equation}
    M\frac{\text{d}^2\vec{R}}{\text{d} t^2} = -\frac{\partial \mathscr{H}_0}{\partial\vec{R}} + <\vec{F}_b> + \text{noise}
\end{equation}
where the first term is due to the physics of the colloid itself, the second one corresponds to the drag, and the last term corresponds to the random action of the water particles on the body. 

\section{Master equation, again}
\subsection{For one and several continuous variables}

We now denote a config $\mathscr{C}$ by a $d$-dimensional vector $x$. We will often assume $d=1$, as the generalisation can be done easily. The master equation writes
\begin{equation}
    \partial_t P(x, t) = \int \text{d}x' w(x' \rightarrow x) P(x', t) - \int \text{d}x' w(x \rightarrow x') P(x, t)
\end{equation}
We choose to write $w$ as a function of the length $w(x \rightarrow x') = w(x, x' - x)$.
\begin{equation}
    \partial_t P(x, t) = \int \text{d}r w(x-r, r) P(x-r, t) - \int \text{d}r w(x, r) P(x, t)
\end{equation}
We expand $w$ for low $r$
\begin{equation}
    \partial_t P(x, t) = \sum_{n\geq 1} \frac{(-1)^n}{n!} \partial^n_x \left[\int \text{d}r~ r^n w(x, r) P(x, t)\right]
\end{equation}
So defining 
\begin{equation}
    a_n(x) = \int \text{d}r ~ r^n w(x, r)
\end{equation}
The master equation rewrites 
\begin{equation}
    \partial_t P =  \sum_{n\geq 1} \frac{(-1)^n}{n!} \partial^n_x (a_n(x)P(x, t))
\end{equation}
So as expected from a conserved current, we have 
\begin{equation}
    \partial_t P = -\partial_\mu J^\mu
\end{equation}
Where 
\begin{equation}
    J^\mu = \text{ probability current}
\end{equation}
which is defined up to a quantity. There is a name for $a_n$: it is the Kramers-Moyal coefficient of order $n$. 

\subsection{Infinitesimal jumps}
Assume that at $t=t_0$, the system is at $x(t_0) = x_0$. Then rum over the dynamics over $[t_0, t_0 + \Delta t]$. Them ask about the random variable 
\begin{equation}
    \Delta x = x(t_0 + \Delta t) - x(t_0)
\end{equation}
when $\Delta t$ is small. We certainly want to solve 
\begin{equation}
    \partial_t P = -\partial_x J
\end{equation}
with $p(x, t | x_0, t_0)$ verifying 
\begin{equation}
    p(x, t_0 | x_0, t_0) = \delta(x - x_0)
\end{equation}
\begin{equation}
    <\Delta x^k> = \int \text{d}x (x-x_0)^k p(x, t_0 + \Delta t | x_0, t_0)
\end{equation}
where 
\begin{equation}
    p(x, t_0 + \Delta t | x_0, t_0) = p(x, t_0 | x_0, t_0) + \Delta t \partial_t p + O(\Delta t^2)
\end{equation}
So $\forall k \geq 1$
\begin{equation}
    <\Delta x^k> = \Delta t \sum \frac{1}{n!}\int \text{d}x \frac{\text{d}^n}{\text{d}x^n} (x-x_0)^k \delta(x - x_0)a(x_0)
\end{equation}
Which gives 
\begin{equation}
    \frac{<\Delta x^k>}{\Delta t} =_{\Delta t \rightarrow 0} a_k(x_0)
\end{equation}
So $a_k(x_0)$ is the $k^{\text{th}}$ momentum of the displacement over $[t_0, t_0 + \Delta t]$ 

\subsection{Approximation}
If we think of the example of the colloid, we expect $\Delta x$ to be small with respect to the scale over which the colloid evolves. \par \medskip 

As a first idea, we can truncate 
\begin{equation}
    \partial_t p = -\partial_x J = -partial_x a_1(x)p(x, t)
\end{equation}
because 
\begin{equation}
    a_n(x) = \int \text{d}r ~ r^n w(x, r)
\end{equation}
is small. At this level of truncation we end up on a deterministic propagation equation in which the probability cloud moves as a block. If we truncate at order 2:
\begin{equation}
    \partial_t  p = -\partial_x (a_1(x)p) + \frac{1}{2}\partial^2(a_2(x)p)
\end{equation}
This is a diffusion equation: Fokker-Planck equation. In high dimension, we see a diffusion equation on a curved spacetime, where the metric of the spacetime is encoded in $a_2$
\begin{equation}
    \partial_t  p = -\partial_\mu (a_1^\mu p) + \frac{1}{2}\partial_\mu\partial_\nu(a_2^{\mu\nu} p)
\end{equation}
However, we cannot expand at higher order: if we truncate at order $k\geq 3$, the positivity of $p$ is not ensured. This is very important, since $p$ is a probability. Hence, we have no choice but to keep our expansion (very limited) at order 1 or 2, or to keep the whole equation.

\section{Langevin's equation}
\subsection{Gaussian random variables}

A Gaussian random variable $\xi$ has a probability density 
\begin{equation}
    P(\vec{\xi}) = \frac{1}{Z} e^{-\frac{1}{2}\xi^\dagger \Gamma \xi}
\end{equation}
with $\Gamma$ a positive definite symmetric matrix, and 
\begin{equation}
    Z = (2\pi)^{M/2}/\sqrt{\det \Gamma}
\end{equation}
where $M$ is the dimension of $\vec{\xi}$. 
With this we can define 
\begin{equation}
    G[h] = <e^{h^\dagger \xi }> = e^{\frac{1}{2}h^\dagger \Gamma^{-1}h}
\end{equation}

We know picture a Gaussian variable as a discrete time process in which $m=1,\dots, M$ is viewed as a time slice index: $t_m = m\Delta t$. Eventually, we want to take the $\Delta t \rightarrow 0$ limit, or $M \rightarrow +\infty$, while keeping $t_\text{obs} = M\Delta t = \text{fixed}$. Hence, we write 
\begin{equation}
    \begin{aligned}
        \text{d}^M\xi P(\xi) &= Z^{-1} \text{d}\xi^1\dots \xi_M e^{-\frac{1}{2} \sum_{mn} \xi_m \Gamma_{mn}\xi_n} \\
        &=  Z^{-1} \text{d}\xi^1\dots \xi_M e^{-\frac{1}{2}\int \text{d}t\text{d}t' \xi(t)\Gamma(t, t')\xi(t')}
    \end{aligned}
\end{equation} 
with $t = m\Delta t$, $\xi(t) = \frac{1}{\sqrt{\Delta t}}\xi_t$, $\Gamma(t, t') = \frac{1}{\Delta t}\Gamma_{tt'}$. \par \medskip 

Usually, one writes 
\begin{equation}
    \frac{1}{Z}\text{d}\xi^1\dots \xi_M = \mathcal{D}\xi
\end{equation}
The matrix $\Gamma(t, t')$ is called a noise kernel, and if $G(t, t')$ (Green's propagator) is its inverse:
\begin{equation}
    \int \text{d}t'' \Gamma(t, t'')G(t'', t') = \delta (t-t')
\end{equation}
where we have introduced 
\begin{equation}
    G(t, t') = \frac{G_{t/\Delta t, t'/\Delta t}}{\Delta t} = <\xi(t), \xi(t')>
\end{equation}
If the $\Delta t \rightarrow 0$ limit exists when computing averagesand correlations of physical quantities, $\xi(t)$ is called a Gaussian process. \par \medskip 

One exemple that will come back very often is one for which 
\begin{equation}
    \Gamma(t, t') = G(t, t') = \delta(t-t')
\end{equation}
In this case, $\xi$ is a Gaussian white noise. For instance, if we are interested in 
\begin{equation}
    x_M = \Delta t^{1/2} \sum_{m=1}^M \xi_m 
\end{equation}
then for $\Gamma_{nm} = \delta_{nm}$ we have 
\begin{equation}
    \begin{aligned}
        &<x_M> = 0 \\
        &<x^2_M> = M\Delta t = t_{\text{obs}}
    \end{aligned}
\end{equation}
This can be recovered with the continuum limit 
\begin{equation}
    x(t_{\text{obs}}) = \int_{0}^{t_{\text{obs}}}\text{d}s \xi(s)
\end{equation}

\subsection{Stochastic Differential Equation}
Let $x_0$ be fixed, and define $x_n$ via the following recursion relation 
\begin{equation}
    x_{n+1} = x_n + f(x_n)\Delta t + g(x_n)\sqrt{\Delta t} \eta_n
\end{equation}
with $f, g$ given functions, and $\eta_n$ a Gaussian process such that $<\eta_n\eta_m> = \delta_{nm}$. \par \medskip 

In the continuum limit, this reads, with $x(t) = x_{t/\Delta t}$
\begin{equation}
    \Delta x = x(t + \Delta t) - x(t) = f(x(t))\Delta t + g(x(t))\sqrt{\Delta t}\eta_m 
\end{equation}
where 
\begin{equation}
    \sqrt{\Delta t}\eta_m  = \Delta\eta = \int_{t}^{t + \Delta t}\text{d}s~ \eta(s)
\end{equation}
with 
\begin{equation}
    \eta(s) = \frac{\eta_{s/\Delta t}}{\sqrt{\Delta t}}
\end{equation}
$\Delta \eta$ is Gaussian with variance $\Delta t$
\begin{equation}
    \begin{aligned}
        &<\Delta \eta> = 0 \\
        &<\Delta \eta ^2> = \int_{t}^{t+\Delta t} \text{d}s\text{d}s' <\eta(s)\eta(s')> = \Delta t
    \end{aligned}
\end{equation}
It is tempting and natural to write 
\begin{equation}
    \frac{\text{d}x}{\text{d}t} = f(x(t)) + g(x(t))\eta(t)
\end{equation}
We immediatly see that it is NOT fine to assume $x$ is differentiable: 
\begin{equation}
    \frac{\Delta x}{\Delta t} \sim O(\frac{1}{\sqrt{\Delta t}})
\end{equation}
it explodes. The continuum limit of 
\begin{equation}
    \Delta x = f(x(t)) \Delta t + g(x(t)) \Delta \eta \Rightarrow \dot{x} = f + g\eta
\end{equation}
is full of traps. Suppose we are given $\alpha \in [0,1]$, then 
\begin{equation}
    \Delta x = f(x(t) + \alpha \Delta x) \Delta t + g(x(t) + \alpha \Delta x)\Delta \eta
\end{equation}
then it looks like as $\Delta t \rightarrow 0$, $\dot{x} = f + g\eta$. So given an equation of the form $\dot{x} = f + g\eta$, we do not have a way of a priori knowing what is the intended discretized form, so we make it explicit by dressing the $=$ sign with $\alpha$

\subsection{Infinitesimal jumps}
Let's focus on the statistics of the infinitesimal jumps $\Delta x = x(t_0 + \Delta t) - x_0$, with $x(t_0) = x_0 = \text{fixed}$. We assume that 
\begin{equation}
    \begin{aligned}
        &\Delta x = f(x(t_0))\Delta t + g(x(t_0))\Delta \eta \\
        &\Rightarrow <\Delta x> = f(x(t_0)) \Delta t \Rightarrow \lim_{\Delta t \rightarrow 0} \frac{<\Delta x>}{\Delta t} = f(x_0) \\ 
        &\Rightarrow \lim_{\Delta t \rightarrow 0} \frac{<\Delta x^2>}{\Delta t} = g^2(x_0)
    \end{aligned}
\end{equation}
Moreover, we can compute that for any $k\geq 3$, we have $ \lim_{\Delta t \rightarrow 0} \frac{<\Delta x^k>}{\Delta t} = 0$ \par \medskip

We know that the statistical properties of $x(t)$ are identical to those of a process governed by the following master's equation 
\begin{equation}
    \partial_t p = -\partial_x (a_1 (x)p ) + \frac{1}{2}\partial^2_x (a_2(x)p)
\end{equation}
in which we have chosen 
\begin{equation}
    a_1(x) = f(x) \qquad a_2(x) = g^2(x)
\end{equation}

\subsection{Stochastic calculus}

Since there is a variety of discretized proesses that lead to the same (visually speaking) continuous SDE (stochastic differential equation), we'd like to understand better the connections between these various discretizations. We shall now take $0 \leq \alpha \leq 1$, and consider 
\begin{equation}
    \begin{aligned}
        &\Delta x = x(t + \Delta t) - x(t) = f(x(t) + \alpha\Delta t) \Delta t + g(x(t) + \alpha \Delta x)\Delta \eta \\
        &\Leftrightarrow_{\Delta t \rightarrow 0} \dot{x} =^\alpha f + g \eta
    \end{aligned}  
\end{equation}
With 
\begin{equation}
    \Delta \eta = \int_{t}^{t+\Delta t}\text{d}s~ \eta(s)
\end{equation}
Our goal is to find, if it exists, the corresponding Fokker-Planck equation. 
\begin{equation}
    \lim_{\Delta t \rightarrow 0} \frac{<\Delta x^k>}{\Delta t} = a_k 
\end{equation}
\begin{equation}
    \Delta x = x(t_0 + \Delta t ) - x(t_0)
\end{equation}
\begin{equation}
    \begin{aligned}
        \Rightarrow ~~&<\Delta x> = <f(x_0 + \alpha \Delta x)\Delta t + g(x_0 + \alpha \Delta x)\Delta \eta>\\ 
        &= <f(x_0) \Delta t + \alpha \Delta x \Delta t f'(x_0) + \frac{1}{2} \alpha^2 \Delta x^2f''(x_0)\Delta t + \dots > \\ 
        &+ <[g(x_0) + \alpha \Delta x g'(x_0) + \dots]\Delta \eta> \\
        & = f(x_0)\Delta t + g(x_0)<\Delta \eta> + \alpha g'(x_0)<\Delta x \Delta \eta> + O(\Delta t ^{3/2}) 
    \end{aligned}
\end{equation}
We also compute
\begin{equation}
    <\Delta x \Delta \eta> = g(x_0) <\Delta \eta^2> + O(\Delta t^{3/2})
\end{equation}
knowing
\begin{equation}
    <\Delta \eta> = 0, \qquad <\Delta \eta^2> = \Delta t
\end{equation}
We get 
\begin{equation}
    \lim_{\Delta t \rightarrow 0} \frac{<\Delta x>}{\Delta t} = f(x_0) + \alpha g'(x_0)g(x_0)
\end{equation}
We carry out the same reasonning for $\Delta x^2$:
\begin{equation}
    \lim_{\Delta t \rightarrow 0}\frac{<\Delta x^2>}{\Delta t} = g^2(x_0)
\end{equation}
And for all $k \geq 3$, 
\begin{equation}
    \lim_{\Delta t \rightarrow 0}\frac{<\Delta x^k>}{\Delta t} = 0
\end{equation}


Hence when we write 
\begin{equation}
    \dot{x} =^\alpha f + g\eta
\end{equation}
We see that it's fully equivalent to 
\begin{equation}
    \dot{x} =^0 f + \alpha g'g + g\eta
\end{equation}
Even if both are visually different, they encode the same physics. Both are described by the same Fokker-Planck equation 
\begin{equation}
    \partial_t \rho = -\partial_x [(f(x) + \alpha g'(x)g(x))\rho] + \frac{1}{2}\partial_x^2 (g^2(x)\rho)
\end{equation}
When $g$ is a constant,
\begin{equation}
    \begin{aligned}
        \dot{x} &=^\alpha  f + g\eta \\
        &=^0 f + g\eta
    \end{aligned}
\end{equation}
This is an additive (noise) Langevin equation, but contrast to g(x) not a constant, which leads to a multiplicative Langevin equation. \par \bigskip 

Let $u(t)$ be a random process slaved to $x(t)$:
\begin{equation}
    u(t) = U (x(t))
\end{equation}
Our goal: find a SDE for $u$ knowing $\dot{x} =^\alpha f + g\eta$. We begin with 
\begin{equation}
    \begin{aligned}
        \Delta u &= u(t_0 + \Delta t) - u(t_0) = U(x_0 + \Delta x) - U(x_0) \\
        &= \Delta x U'(x_0) + \frac{1}{2} U''(x_0)\Delta x^2 + O(\Delta t^{3/2}) \\
        &= [f(x_0 + \alpha \Delta x)\Delta t + g(x_0 + \alpha \Delta x)\Delta \eta]U'(x_0) \\
        &~~~+ \frac{1}{2}U''(x_0)[f(x_0 + \alpha \Delta x)\Delta t + g(x_0 + \alpha \Delta x)\Delta \eta]^2 + O(\Delta t^{3/2})
    \end{aligned}
\end{equation}

Such that 
\begin{equation}
    \begin{aligned}
        \Delta u = &f(x_0)\Delta t U'(x_0) \\
        &+g(x_0) U'(x_0)\Delta \eta \\
        &+\alpha g'(x_0)U'(x_0)\Delta x\Delta \eta \\
        &+ \frac{1}{2} U''(x_0)g^2(x_0)\Delta \eta ^2 \\
        &+ O(\Delta t ^{3/2})
    \end{aligned}
\end{equation}
So
\begin{equation}
    \lim_{\Delta t \rightarrow 0}\frac{<\Delta u>}{\Delta t} = f(x_0)U'(x_0) + \alpha g'gU' + \frac{1}{2}U'' g^2
\end{equation}
\begin{equation}
    \lim_{\Delta t \rightarrow 0}\frac{<\Delta u^2>}{\Delta t} = U'(x_0)^2\Delta x^2 + O(t^{3/2})
\end{equation}
And for all $k \geq 3$, 
\begin{equation}
    \lim_{\Delta t \rightarrow 0}\frac{<\Delta u^k>}{\Delta t} = 0
\end{equation}
We could now write a Fokker-Planck equation for 
\begin{equation}
    q(u, t) = \text{Prob}\{ u \leq u(t) \leq u+\text{d}u\}/\text{d}u
\end{equation}
\begin{equation}
    \partial_t q = -\frac{\partial}{\partial u}\left([(f + \alpha g'g)U + \frac{1}{2}g^2U'']q\right) + \frac{1}{2}\partial_u^2(g^2U^{'2} q)
\end{equation}
where $x$ has to be expressed in term of $u$. \par \medskip 
Hence, if $\dot{x}=^\alpha f + g\eta$ then 
\begin{equation}
    \frac{\text{d}u}{\text{d}t} =^0 (f + \alpha g'g)U' + \frac{1}{2}g^2 U'' + gU'\eta
\end{equation}
If we had started from $\alpha = 0$
\begin{equation}
    \dot{x} =^0 f + g\eta
\end{equation}
then 
\begin{equation}
    \dot{u} =^0 \dot{x}U' + \frac{g^2U''}{2}
\end{equation}
It$\bar{o}$'s lemma. But if $\alpha \neq 0$, $\dot{x}=^\alpha f + g\eta$ leads to 
\begin{equation}
    \begin{aligned}
        \dot{u} &=^0 (f + \alpha g'g)U' + \frac{1}{2}g^2 U'' + gU'\eta \\
        &=^{\alpha'} (f + \alpha g'g)U' + \frac{1}{2}g^2 U'' + gU'\eta - \alpha' \frac{\text{d}}{\text{d}u}\left(gU'\right)gU'\\
        &=^{\alpha'} (f + (\alpha-\alpha') g'g)U' + (\frac{1}{2}-\alpha)g^2 U'' + gU'\eta 
    \end{aligned}
\end{equation}
\begin{equation}
    \dot{u} =^{\alpha} \dot{x}U' + (\frac{1}{2}- \alpha)g^2U''
\end{equation}

On this version of the It$\bar{o}$'s lemma, we see explicitely that the chain rule fails. But we also see that $\alpha = \frac{1}{2}$ plays a particular role where formally, the chain rule holds 
\begin{equation}
    \begin{aligned}
        &\dot{x} =^{\frac{1}{2}} f + g\eta \\ 
        &\dot{u} =^{\frac{1}{2}} \dot{x}U'
    \end{aligned}
\end{equation}

"Choices" of $\alpha$ : 
\begin{itemize}
    \item $\alpha = 0$: (prepoint) Ito discretization, very nice for computers, but we lose differential calculus (no chain rule)
    \begin{equation}
        x_{n+1} = x_n + f(x_n)\Delta t + g(x_n)\sqrt{\Delta t}\eta_n, \quad <\eta_n\eta_m> = \delta_{n,m}
    \end{equation}
    \item $\alpha = \frac{1}{2}$: (midpoint) Statonovich discretization: not cool, implicit recursion, almost impossible to implement on a computer. But cool, we have chain rule. Even cooler, physics consistent. It is also the only one consistent with time reversal
    \item $\alpha = 1$: (postpoint) Klimontovich-Hanggi, nice for plasma physics. We want to write everything in path integral formalism, and use Matsubara time, which uses post point discretization
\end{itemize}

Of course, all of this extends to 
\begin{equation}
    \dot{x}^\mu =^\alpha f^\mu + g^{\mu i}\eta_i
\end{equation}
with $\mu = 1,\dots, d$ and $i = 1, \dots, d' \leq d$. $u = U^\mu(x)$ evolves according to 
\begin{equation}
    \dot{u}^\mu =^0 \partial_\nu U^\mu f^\nu + \partial_\nu U^\mu g^{\nu i}\eta_i + \frac{1}{2}\partial_\beta \partial_\gamma U^\mu \omega^{\beta\gamma}
\end{equation}
with $\omega^{\beta\gamma} = g^{\beta i}g^{\gamma i}$. 

\section{Connection to physics}
What is the connection between $\dot{x} = f + g\eta$ and physics? Let's go back to the colloid in water picture. 
\begin{equation}
    M\frac{\text{d}\vec{P}}{\text{d}t} = \vec{F}_b + \vec{F}_{\text{ext}}
\end{equation}

We separate the bath into regions 
\begin{equation}
    \begin{aligned}
        \vec{F}_b &= \sum \vec{F}_i, \quad \vec{F}_i = \text{ force created by region }i \\
        &= <\vec{F}_b> + \text{ Gaussian fluctuations}
    \end{aligned}
\end{equation}
where $<\vec{F}_b>$ is an average over the baths at fixed $\vec{R}, \vec{P}$. \par \medskip 
We try to write something as general as possible, at the linear level
\begin{equation}
    <\vec{F}_b> = -\int \text{d}t'~ M_R(t-t')\frac{\text{d}\vec{R}}{\text{d}t'}
\end{equation}
Where $M_R(t-t')$ can depend functionally on $\vec{R}$. Condensing all of the above assumptions, we can guess
\begin{equation}
    M\frac{\text{d}^2 \vec{R}}{\text{d}t^2} = \vec{F}_{\text{ext}} - \int \text{d}t'~ M_R(t-t')\frac{\text{d}\vec{R}}{\text{d}t'} + \vec{xi}
\end{equation} 
where $M_R(t-t')$ is the response kernel, the response of the environment to the movement of the colloid. On the other hand, we have 
\begin{equation}
    <\xi^\mu(t)\xi^\nu(t)> = TM^{\mu\nu}_c (t-t')
\end{equation}
with $M^{\mu\nu}_c (t-t')$ the correlation kernel. \par \medskip 

There has to be a strong connection between $M_R$ and $M_c$ because they are two faces of the same physical process. If the physics of the system is such that the memory of the bath is quickly forgotten, then it is fine to replace 
\begin{equation}
    M_R(t-t') = \gamma \delta(t-t')
\end{equation}
and 
\begin{equation}
    M_c(t-t') = \gamma' \delta(t-t')
\end{equation}
So that 
\begin{equation}
    M \frac{\text{d}^2 \vec{R}}{\text{d}t^2} = \vec{F}_{\text{ext}} - \gamma \frac{\text{d}\vec{R}}{\text{d}t} + \sqrt{2\gamma' T}\vec{\eta}
\end{equation}
\begin{equation}
    <\eta^\mu(t)\eta^\nu(t)> = \sigma^{\mu\nu}\delta(t-t')
\end{equation}
This is an underdamped Langevin equation. \par \medskip 

For a colloid in water, the overdamped limit is when we can make the approximation that there is an almost instantaneous balance between $\vec{F}_{\text{ext}}$ and $\vec{F}_b$, so that inertia can be neglected
\begin{equation}
    \vec{0} \simeq \vec{F}_{\text{ext}} + \vec{F}_b
\end{equation}

\begin{equation}
    \begin{aligned}
        &-\gamma \frac{\text{d}\vec{R}}{\text{d}t} + \sqrt{2\gamma'T} \eta + \vec{F}_{\text{ext}} = 0 \\
        &\frac{\text{d}\vec{R}}{\text{d}t} = \mu \vec{F}_{\text{ext}} + \sqrt{2\gamma' \mu^2}\vec{\eta}
    \end{aligned}
\end{equation}
with $\mu = \frac{1}{\gamma}$ the mobility.  \par \medskip 

We'll take this sort of equations as a starting point. For instance, for an isolated particle, in the underdamped description
\begin{equation}
    d=1: \qquad  M\frac{\text{d}v}{\text{d}t} = -\gamma v + \sqrt{2\gamma' T }\eta
\end{equation}
where the Markov approximation $M_R \propto \delta(t)$ was used. But we want to derive an evolution equation for 
\begin{equation}
    K = \frac{Mv^2}{2}
\end{equation}
From the one above, assuming we are using a $\alpha = 1/2$ discretization, we get 
\begin{equation}
    \frac{\text{d}K}{\text{d}t} =^{1/2} M v\dot{v} = -\gamma Mv^2 + Mv \sqrt{2\gamma'T}\eta
\end{equation}
or assuming we were using a $\alpha = 0$ discretization, we get 
\begin{equation}
    \frac{\text{d}K}{\text{d}t} =^0 \dot{v}\frac{\text{d}K}{\text{d}v} + \frac{1}{2}\frac{\text{d}^2 K}{\text{d}v^2}2\gamma'T 
\end{equation}
which leads through computations to 
\begin{equation}
    \frac{M<v^2>}{2} = \frac{\gamma'}{\gamma}\frac{T}{2}
\end{equation}
from which we understand that we must have $\gamma' = \gamma$ \par \medskip 

Multiplicative noise also occurs directly for a simple overdamped colloid in water 
\begin{equation}
    \gamma \frac{\text{d}\vec{R}}{\text{d}t} = \sqrt{2\gamma' T}\vec{\eta}
\end{equation}
In a river, $\gamma$ depends on the distance to the wall. 
\chapter{Time-reversal}

\chapter{Metastability}

\chapter{Mean-field}

\chapter{Field theories}

\chapter{Exactly solvable models}

\end{document}